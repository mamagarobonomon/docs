---
title: "Real-time Setup"
description: "Configure Nimbus for ultra-low latency BCI inference"
icon: "gear"
---

# Real-time Inference Setup

Setting up Nimbus for real-time BCI applications requires careful configuration to achieve sub-20ms latency while maintaining accuracy. This guide covers the essential configuration options and best practices for optimal real-time performance.

## Quick Start Configuration

For immediate real-time BCI setup:

```python
import nimbus

# Optimized configuration for real-time BCI
config = nimbus.RealTimeConfig(
    target_latency=15,  # milliseconds
    max_channels=64,    # EEG channels
    sample_rate=1000,   # Hz
    buffer_size=16,     # samples
    precision='float32' # balance speed vs accuracy
)

# Create real-time BCI system
bci_system = nimbus.create_realtime_system(config)
```

## Configuration Options

### Latency Settings

Control the speed-accuracy tradeoff:

<AccordionGroup>
  <Accordion title="Target Latency">
    Set your maximum acceptable latency:
    
    ```python
    config = nimbus.RealTimeConfig(
        target_latency=10,   # Ultra-fast (gaming)
        target_latency=15,   # Fast (general BCI)
        target_latency=20,   # Standard (research)
    )
    ```
    
    **Trade-offs:**
    - Lower latency = Less processing time = Potentially lower accuracy
    - Higher latency = More processing time = Better accuracy
  </Accordion>
  
  <Accordion title="Processing Mode">
    Choose between different processing strategies:
    
    ```python
    # Streaming mode (lowest latency)
    config.processing_mode = 'streaming'
    
    # Micro-batch mode (balanced)
    config.processing_mode = 'micro_batch'
    config.batch_size = 4
    
    # Adaptive mode (automatic optimization)
    config.processing_mode = 'adaptive'
    ```
  </Accordion>
  
  <Accordion title="Buffer Configuration">
    Optimize buffering for your application:
    
    ```python
    config.buffer_config = {
        'input_buffer_size': 32,    # Input samples to buffer
        'output_buffer_size': 16,   # Output predictions to buffer
        'circular_buffer': True,    # Use circular buffering
        'zero_copy': True          # Avoid memory copying
    }
    ```
  </Accordion>
</AccordionGroup>

### Hardware Optimization

Configure Nimbus for your specific hardware:

```python
# Automatic hardware detection and optimization
config = nimbus.RealTimeConfig.auto_optimize(
    cpu_cores=8,
    gpu_memory='4GB',
    target_latency=15
)

# Manual hardware configuration
config.hardware = {
    'use_gpu': True,
    'gpu_device': 0,
    'cpu_threads': 4,
    'memory_pool_size': '1GB',
    'pin_memory': True
}
```

### Signal Processing Pipeline

Configure the processing pipeline for your BCI type:

<Columns cols={2}>
  <Card title="Motor Imagery BCI" icon="gamepad">
    ```python
    config.pipeline = nimbus.MotorImageryPipeline(
        frequency_bands=[(8, 12), (18, 25)],
        spatial_filter='CSP',
        classifier='LDA',
        post_processing='exponential_smoothing'
    )
    ```
  </Card>
  <Card title="P300 Speller" icon="keyboard">
    ```python
    config.pipeline = nimbus.P300Pipeline(
        epoch_length=0.8,  # seconds
        baseline_correction=True,
        artifact_rejection='automatic',
        averaging_method='weighted'
    )
    ```
  </Card>
  <Card title="SSVEP BCI" icon="signal">
    ```python
    config.pipeline = nimbus.SSVEPPipeline(
        target_frequencies=[6, 7.5, 8.57, 10],
        window_length=1.0,  # seconds
        method='CCA',
        harmonics=3
    )
    ```
  </Card>
  <Card title="Custom Pipeline" icon="cog">
    ```python
    config.pipeline = nimbus.CustomPipeline(
        preprocessing_steps=[
            'bandpass_filter',
            'notch_filter',
            'artifact_removal'
        ],
        feature_extraction='custom',
        classifier='probabilistic'
    )
    ```
  </Card>
</Columns>

## Performance Tuning

### Latency Optimization

Fine-tune for minimum latency:

```python
# Ultra-low latency configuration
ultra_fast_config = nimbus.RealTimeConfig(
    target_latency=8,           # Aggressive target
    processing_mode='streaming', # No batching
    precision='float16',        # Reduced precision
    optimization_level='max_speed',
    
    # Disable expensive operations
    uncertainty_quantification=False,
    detailed_logging=False,
    
    # Hardware optimizations
    use_gpu=True,
    pin_memory=True,
    prefetch_factor=2
)
```

### Accuracy Optimization

Prioritize accuracy over speed:

```python
# High accuracy configuration
accurate_config = nimbus.RealTimeConfig(
    target_latency=25,          # More processing time
    processing_mode='micro_batch',
    batch_size=8,
    precision='float64',        # Higher precision
    optimization_level='max_accuracy',
    
    # Enable advanced features
    uncertainty_quantification=True,
    ensemble_methods=True,
    adaptive_learning=True,
    
    # Quality checks
    signal_quality_monitoring=True,
    artifact_detection=True
)
```

### Balanced Configuration

Optimal balance for most applications:

```python
# Balanced configuration (recommended)
balanced_config = nimbus.RealTimeConfig(
    target_latency=15,
    processing_mode='adaptive',
    precision='float32',
    optimization_level='balanced',
    
    # Essential features
    uncertainty_quantification=True,
    signal_quality_monitoring=True,
    
    # Performance optimizations
    use_gpu=True,
    cpu_threads=4,
    memory_optimization=True
)
```

## Initialization and Calibration

### System Initialization

Proper initialization is crucial for real-time performance:

```python
# Initialize the real-time system
bci_system = nimbus.create_realtime_system(config)

# Pre-compile computational graphs
bci_system.compile(
    input_shape=(64, 1000),  # channels x samples
    optimization_passes=['constant_folding', 'dead_code_elimination']
)

# Warm up the system
bci_system.warmup(num_iterations=100)
```

### User Calibration

Calibrate the system for individual users:

```python
# Collect calibration data
calibration_data = collect_calibration_session(
    user_id='user_001',
    session_length=300,  # seconds
    tasks=['left_hand', 'right_hand', 'rest']
)

# Train user-specific model
user_model = bci_system.calibrate(
    data=calibration_data,
    method='supervised',
    cross_validation=True
)

# Update system with user model
bci_system.load_user_model(user_model)
```

### Online Adaptation

Enable continuous learning during use:

```python
# Configure online adaptation
adaptation_config = {
    'enabled': True,
    'learning_rate': 0.01,
    'adaptation_window': 100,  # samples
    'stability_threshold': 0.95,
    'max_drift_tolerance': 0.1
}

bci_system.enable_online_adaptation(adaptation_config)
```

## Monitoring and Diagnostics

### Real-time Metrics

Monitor system performance during operation:

```python
# Enable performance monitoring
monitor = nimbus.RealTimeMonitor(
    metrics=['latency', 'throughput', 'accuracy', 'signal_quality'],
    update_interval=1.0,  # seconds
    alert_thresholds={
        'latency': 20,      # ms
        'accuracy': 0.8,    # minimum
        'signal_quality': 0.7
    }
)

# Start monitoring
monitor.start()

# Get current metrics
current_metrics = monitor.get_metrics()
print(f"Latency: {current_metrics.latency:.1f}ms")
print(f"Accuracy: {current_metrics.accuracy:.2%}")
```

### Diagnostic Tools

Debug performance issues:

```python
# Profile system performance
profiler = nimbus.RealTimeProfiler()
profiler.start()

# Run inference
result = bci_system.infer(eeg_data)

profiler.stop()

# Analyze bottlenecks
bottlenecks = profiler.identify_bottlenecks()
for component, time_ms in bottlenecks.items():
    print(f"{component}: {time_ms:.2f}ms")
```

### Logging Configuration

Configure logging for production systems:

```python
# Production logging configuration
logging_config = {
    'level': 'INFO',
    'real_time_safe': True,     # Avoid blocking operations
    'buffer_size': 1000,        # Log entries to buffer
    'flush_interval': 5.0,      # seconds
    'include_metrics': True,
    'include_predictions': False  # Avoid sensitive data
}

bci_system.configure_logging(logging_config)
```

## Deployment Considerations

### Edge Deployment

Configure for edge computing devices:

```python
# Edge-optimized configuration
edge_config = nimbus.RealTimeConfig(
    target_latency=15,
    memory_limit='512MB',       # Limited memory
    cpu_only=True,              # No GPU available
    model_compression=True,     # Compress models
    quantization='int8',        # Reduce model size
    
    # Minimize resource usage
    background_tasks=False,
    telemetry=False,
    detailed_logging=False
)
```

### Cloud Deployment

Configure for cloud-based inference:

```python
# Cloud-optimized configuration
cloud_config = nimbus.RealTimeConfig(
    target_latency=10,
    use_gpu=True,
    gpu_memory='8GB',
    auto_scaling=True,
    
    # Cloud-specific features
    load_balancing=True,
    health_checks=True,
    metrics_export=True,
    
    # Reliability features
    failover_enabled=True,
    backup_models=True
)
```

### Hybrid Deployment

Combine edge and cloud processing:

```python
# Hybrid configuration
hybrid_config = nimbus.HybridConfig(
    edge_config={
        'target_latency': 10,
        'critical_path_only': True
    },
    cloud_config={
        'advanced_analytics': True,
        'model_updates': True,
        'batch_processing': True
    },
    fallback_strategy='edge_only'
)
```

## Best Practices

### Configuration Management

<Tip>
Use configuration files for reproducible setups and easy deployment across different environments.
</Tip>

```python
# Save configuration
config.save('production_config.json')

# Load configuration
config = nimbus.RealTimeConfig.load('production_config.json')

# Environment-specific overrides
if environment == 'production':
    config.logging_level = 'WARNING'
    config.telemetry = True
elif environment == 'development':
    config.logging_level = 'DEBUG'
    config.profiling = True
```

### Testing and Validation

```python
# Validate configuration before deployment
validation_result = config.validate()
if not validation_result.is_valid:
    print("Configuration issues:")
    for issue in validation_result.issues:
        print(f"- {issue}")

# Benchmark performance
benchmark = nimbus.benchmark_configuration(
    config=config,
    test_data=validation_dataset,
    iterations=1000
)

print(f"Average latency: {benchmark.avg_latency:.1f}ms")
print(f"99th percentile: {benchmark.p99_latency:.1f}ms")
```

## Troubleshooting

### Common Issues

<AccordionGroup>
  <Accordion title="High Latency">
    **Symptoms**: Latency > target threshold
    
    **Solutions**:
    - Reduce buffer sizes
    - Enable GPU acceleration
    - Decrease precision (float32 → float16)
    - Simplify processing pipeline
    - Check system resource usage
  </Accordion>
  
  <Accordion title="Poor Accuracy">
    **Symptoms**: Classification accuracy below expectations
    
    **Solutions**:
    - Increase processing window
    - Enable uncertainty quantification
    - Improve signal quality
    - Recalibrate user model
    - Check for artifacts
  </Accordion>
  
  <Accordion title="System Instability">
    **Symptoms**: Crashes, memory leaks, inconsistent performance
    
    **Solutions**:
    - Enable memory pooling
    - Reduce batch sizes
    - Check hardware compatibility
    - Update drivers
    - Monitor system resources
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="Streaming Inference" icon="signal" href="/inference-configuration/streaming-inference">
    Learn about continuous data processing
  </Card>
  <Card title="Batch Processing" icon="layers" href="/inference-configuration/batch-processing">
    Configure offline analysis workflows
  </Card>
  <Card title="Error Handling" icon="shield" href="/inference-configuration/error-handling">
    Implement robust error recovery
  </Card>
  <Card title="Examples" icon="list" href="/examples/basic-examples">
    See real-time setup in action
  </Card>
</CardGroup>

---

<Note>
**Next**: Learn how to configure [streaming inference](/inference-configuration/streaming-inference) for continuous BCI data processing.
</Note>

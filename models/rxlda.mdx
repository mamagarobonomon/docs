---
title: "Bayesian LDA (RxLDA)"
description: "Bayesian Linear Discriminant Analysis with uncertainty quantification using RxInfer.jl"
icon: "brain"
---

# Bayesian LDA (RxLDA) - Bayesian Linear Discriminant Analysis

**API Name**: `RxLDAModel`  
**Mathematical Model**: Pooled Gaussian Classifier (PGC)

Bayesian LDA (also known as RxLDA in the codebase) is a Bayesian classification model with uncertainty quantification, implemented using RxInfer.jl's reactive message passing framework. It uses a **shared precision matrix** across all classes, making it fast and efficient for BCI classification.

<Note>
**Bayesian LDA (RxLDA) is currently implemented** in NimbusSDK.jl and ready for use in production BCI applications. LDA is widely recognized in the BCI community, and "Bayesian" signals our uncertainty quantification capabilities.
</Note>

## Overview

Bayesian LDA extends traditional Linear Discriminant Analysis with full Bayesian inference, providing:

- ✅ **Posterior probability distributions** (not just point estimates)
- ✅ **Uncertainty quantification** for each prediction
- ✅ **Probabilistic confidence scores**
- ✅ **Fast inference** (&lt;20ms per trial)
- ✅ **Training and calibration** support
- ✅ **Batch and streaming** inference modes

### When to Use Bayesian LDA

**Bayesian LDA is ideal for:**
- Motor Imagery classification (2-4 classes)
- Well-separated class distributions
- Fast inference requirements (&lt;20ms)
- Interpretable results for medical applications
- When classes have similar covariance structures

**Consider [Bayesian GMM](/models/rxgmm) instead if:**
- Classes have significantly different covariance structures
- Complex, overlapping distributions
- Need more flexibility in modeling per-class variances

## Model Architecture

### Mathematical Foundation (Pooled Gaussian Classifier)

Bayesian LDA implements a Pooled Gaussian Classifier (PGC), which models class-conditional distributions with a shared precision matrix:

```
p(x | y=k) = N(μ_k, W^-1)
```

Where:
- `μ_k` = mean vector for class k (learned from data)
- `W` = **shared** precision matrix (same for all classes)
- Assumes classes have similar covariance structure

**Key Assumption**: All classes share the same covariance structure, which makes training and inference faster.

### Hyperparameters (v0.2.0+)

Bayesian LDA supports configurable hyperparameters for optimal performance tuning:

**Available Hyperparameters:**

| Parameter | Type | Default | Range | Description |
|-----------|------|---------|-------|-------------|
| `dof_offset` | Int | 2 | [1, 5] | Degrees of freedom offset for Wishart prior |
| `mean_prior_precision` | Float64 | 0.01 | [0.001, 0.1] | Prior precision for class means |
| `predictive_mean_prior` | Float64 | 1e6 | [1e4, 1e8] | Mean prior strength during inference |
| `predictive_dof_offset` | Int | 2 | [1, 5] | DOF offset for inference (must equal `dof_offset`) |

**Parameter Effects:**

- **dof_offset**: Controls regularization strength
  - Lower values (1) → More data-driven, less regularization
  - Higher values (3-5) → More regularization, more conservative
  
- **mean_prior_precision**: Controls prior strength on class means
  - Lower values (0.001) → Weaker prior, trusts data more
  - Higher values (0.05-0.1) → Stronger prior, more regularization

- **predictive_mean_prior**: Very high values (1e6) indicate strong trust in learned parameters during inference

- **predictive_dof_offset**: Must match `dof_offset` (enforced by validation)

<Note>
**New in v0.2.0**: Hyperparameter configuration allows you to optimize model behavior for your specific dataset characteristics (SNR, trial count, data quality).
</Note>

### Model Structure

```julia
struct RxLDAModel <: BCIModel
    means::Vector{Vector{Float64}}            # Class means [μ₁, μ₂, ..., μₖ]
    precisions::Vector{Matrix{Float64}}       # Precision matrices (all equal to W)
    global_precision::Matrix{Float64}         # Shared precision W
    metadata::ModelMetadata                   # Model info
    dof_offset::Int                          # Degrees of freedom offset
    mean_prior_precision::Float64            # Mean prior precision
    predictive_mean_prior::Float64           # Predictive mean prior
    predictive_dof_offset::Int               # Predictive DOF offset
end
```

### RxInfer Implementation

The Bayesian LDA model uses RxInfer.jl for variational Bayesian inference:

**Learning Phase:**
```julia
@model function RxLDA_learning_model(y, labels, n_features, n_classes, dof_offset, mean_prior_precision)
    # Prior on shared precision
    dof = n_features + dof_offset
    W ~ Wishart(dof, I)
    
    # Priors on class means
    for k in 1:n_classes
        m[k] ~ MvNormal(mean=zeros(n_features), precision=mean_prior_precision * I)
    end
    
    # Likelihood with known labels
    for i in eachindex(y)
        k = labels[i]
        y[i] ~ MvNormal(mean=m[k], precision=W)
    end
end
```

**Prediction Phase (Per-Class Likelihood):**

To avoid mixture collapse with single observations, inference computes per-class likelihoods independently:

```julia
@model function RxLDA_predictive_single_class(y, class_mean, class_precision, predictive_mean_prior, predictive_dof_offset)
    n_features = length(class_mean)
    
    # Prior for this specific class
    m ~ MvNormalMeanPrecision(class_mean, predictive_mean_prior * I)
    w ~ Wishart(n_features + predictive_dof_offset, class_precision)
    
    # Observations (no mixture variable)
    for i in eachindex(y)
        y[i] ~ MvNormal(mean=m, precision=w)
    end
end
```

For each class k, compute p(y | μ_k, W) independently, then combine using Bayes' rule with softmax normalization.

## Usage

### 1. Load Pre-trained Model

```julia
using NimbusSDK

# Authenticate
NimbusSDK.install_core("nbci_live_your_key")

# Load from Nimbus model zoo
model = load_model(RxLDAModel, "motor_imagery_4class_v1")

println("Model loaded:")
println("  Features: $(get_n_features(model))")
println("  Classes: $(get_n_classes(model))")
println("  Paradigm: $(get_paradigm(model))")
```

### 2. Train Custom Model

```julia
using NimbusSDK

# Prepare training data with labels
train_features = csp_features  # (16 × 250 × 100)
train_labels = [1, 2, 3, 4, 1, 2, ...]  # 100 labels

train_data = BCIData(
    train_features,
    BCIMetadata(
        sampling_rate = 250.0,
        paradigm = :motor_imagery,
        feature_type = :csp,
        n_features = 16,
        n_classes = 4,
        chunk_size = nothing
    ),
    train_labels  # Required for training!
)

# Train RxLDA model with default hyperparameters
model = train_model(
    RxLDAModel,
    train_data;
    iterations = 50,        # Inference iterations
    showprogress = true,    # Show progress bar
    name = "my_motor_imagery",
    description = "4-class MI classifier with CSP"
)

# Or train with custom hyperparameters (v0.2.0+)
model = train_model(
    RxLDAModel,
    train_data;
    iterations = 50,
    showprogress = true,
    name = "my_motor_imagery_tuned",
    description = "4-class MI with tuned hyperparameters",
    dof_offset = 2,                    # DOF offset (default: 2)
    mean_prior_precision = 0.01,       # Prior precision (default: 0.01)
    predictive_mean_prior = 1e6,       # Inference prior (default: 1e6)
    predictive_dof_offset = 2          # Inference DOF (default: 2)
)

# Save for later use
save_model(model, "my_model.jld2")
```

**Training Parameters:**
- `iterations`: Number of variational inference iterations (default: 50)
  - More iterations = better convergence but slower training
  - 50-100 is typically sufficient
- `showprogress`: Display progress bar during training
- `name`: Model identifier
- `description`: Model description for documentation
- `dof_offset`: Degrees of freedom offset (default: 2, range: [1, 5]) - **v0.2.0+**
- `mean_prior_precision`: Prior precision for means (default: 0.01, range: [0.001, 0.1]) - **v0.2.0+**
- `predictive_mean_prior`: Inference prior strength (default: 1e6, range: [1e4, 1e8]) - **v0.2.0+**
- `predictive_dof_offset`: Inference DOF offset (default: 2, range: [1, 5], **must equal `dof_offset`**) - **v0.2.0+**

### 3. Subject-Specific Calibration

Fine-tune a pre-trained model with subject-specific data (much faster than training from scratch):

```julia
# Load base model
base_model = load_model(RxLDAModel, "motor_imagery_baseline_v1")

# Collect 10-20 calibration trials from new subject
calib_features = collect_calibration_trials()  # Your function
calib_labels = [1, 2, 3, 4, 1, 2, ...]

calib_data = BCIData(calib_features, metadata, calib_labels)

# Calibrate (personalize) the model
personalized_model = calibrate_model(
    base_model,
    calib_data;
    iterations = 20  # Fewer iterations needed
)

save_model(personalized_model, "subject_001_calibrated.jld2")
```

**Calibration Benefits:**
- Requires only 10-20 trials per class (vs 50-100 for training from scratch)
- Faster: 20 iterations vs 50-100
- Better generalization: Uses pre-trained model as prior
- Typical accuracy improvement: 5-15% over generic model
- **Hyperparameters automatically preserved**: `calibrate_model()` inherits all hyperparameters from the base model (`dof_offset`, `mean_prior_precision`, `predictive_mean_prior`, `predictive_dof_offset`) ensuring consistency (v0.2.0+)

<Note>
**Important**: You don't need to specify hyperparameters when calibrating - they are automatically inherited from the base model. This ensures the calibrated model uses the same regularization strategy that worked well during initial training.
</Note>

### 4. Batch Inference

Process multiple trials efficiently:

```julia
# Prepare test data
test_data = BCIData(test_features, metadata, test_labels)

# Run batch inference
results = predict_batch(model, test_data; iterations=10)

# Analyze results
println("Predictions: ", results.predictions)
println("Mean confidence: ", mean(results.confidences))

# Calculate accuracy
accuracy = sum(results.predictions .== test_labels) / length(test_labels)
println("Accuracy: $(round(accuracy * 100, digits=1))%")

# Calculate ITR
itr = calculate_ITR(accuracy, 4, 4.0)  # 4 classes, 4-second trials
println("ITR: $(round(itr, digits=1)) bits/minute")
```

### 5. Streaming Inference

Real-time chunk-by-chunk processing:

```julia
# Initialize streaming session
session = init_streaming(model, metadata_with_chunk_size)

# Process chunks as they arrive
for chunk in eeg_feature_stream
    result = process_chunk(session, chunk; iterations=10)
    println("Chunk: pred=$(result.prediction), conf=$(round(result.confidence, digits=3))")
end

# Finalize trial with aggregation
final_result = finalize_trial(session; method=:weighted_vote)
println("Final: pred=$(final_result.prediction), conf=$(round(final_result.confidence, digits=3))")
```

## Hyperparameter Tuning (v0.2.0+)

Fine-tune Bayesian LDA for optimal performance on your specific dataset.

### When to Tune Hyperparameters

Consider tuning when:
- Default performance is unsatisfactory
- You have specific data characteristics (very noisy or very clean)
- You have limited or extensive training data
- You want to optimize for your specific paradigm

### Tuning Strategies

#### For High SNR / Clean Data / Many Trials

Use lower regularization to let the data drive the model:

```julia
model = train_model(
    RxLDAModel,
    train_data;
    iterations = 50,
    dof_offset = 1,                    # Less regularization
    mean_prior_precision = 0.001,      # Weaker prior, trust data more
    predictive_mean_prior = 1e6,       # Standard inference prior
    predictive_dof_offset = 1          # Match dof_offset
)
```

**Use when:**
- SNR > 5 dB
- 100+ trials per class
- Clean, artifact-free data
- Well-controlled experimental conditions

#### For Low SNR / Noisy Data / Few Trials

Use higher regularization for stability:

```julia
model = train_model(
    RxLDAModel,
    train_data;
    iterations = 50,
    dof_offset = 3,                    # More regularization
    mean_prior_precision = 0.05,       # Stronger prior
    predictive_mean_prior = 1e6,       # Standard inference prior
    predictive_dof_offset = 3          # Match dof_offset
)
```

**Use when:**
- SNR < 2 dB
- 40-80 trials per class
- Noisy data or limited artifact removal
- Challenging recording conditions

#### Balanced / Default Settings

The defaults work well for most scenarios:

```julia
model = train_model(
    RxLDAModel,
    train_data;
    iterations = 50,
    dof_offset = 2,                    # Balanced (default)
    mean_prior_precision = 0.01,       # Balanced (default)
    predictive_mean_prior = 1e6,       # Standard (default)
    predictive_dof_offset = 2          # Balanced (default)
)
```

**Use when:**
- Moderate SNR (2-5 dB)
- 80-150 trials per class
- Standard BCI recording conditions
- Starting point for experimentation

### Hyperparameter Search Example

Systematically search for optimal hyperparameters:

```julia
using NimbusSDK

# Define search grid
dof_values = [1, 2, 3, 4]
prior_values = [0.001, 0.01, 0.05, 0.1]

# Split data into train/validation
train_data, val_data = split_data(all_data, ratio=0.8)

best_accuracy = 0.0
best_params = nothing

println("Searching hyperparameters...")
for dof in dof_values
    for prior in prior_values
        # Train model with these hyperparameters
        model = train_model(
            RxLDAModel,
            train_data;
            iterations = 50,
            dof_offset = dof,
            mean_prior_precision = prior,
            predictive_dof_offset = dof,
            showprogress = false
        )
        
        # Validate
        results = predict_batch(model, val_data)
        accuracy = sum(results.predictions .== val_data.labels) / length(val_data.labels)
        
        println("  dof=$dof, prior=$prior: $(round(accuracy*100, digits=1))%")
        
        # Track best
        if accuracy > best_accuracy
            best_accuracy = accuracy
            best_params = (dof=dof, prior=prior)
        end
    end
end

println("\nBest hyperparameters:")
println("  dof_offset: $(best_params.dof)")
println("  mean_prior_precision: $(best_params.prior)")
println("  Validation accuracy: $(round(best_accuracy*100, digits=1))%")

# Retrain on all data with best hyperparameters
final_model = train_model(
    RxLDAModel,
    all_data;
    iterations = 50,
    dof_offset = best_params.dof,
    mean_prior_precision = best_params.prior,
    predictive_dof_offset = best_params.dof
)
```

### Quick Tuning Guidelines

| Scenario | `dof_offset` | `mean_prior_precision` | Notes |
|----------|--------------|------------------------|-------|
| **Excellent data quality** | 1 | 0.001 | Minimal regularization |
| **Good data quality** | 2 (default) | 0.01 (default) | Balanced approach |
| **Moderate data quality** | 2-3 | 0.01-0.03 | Slight regularization |
| **Poor data quality** | 3-4 | 0.05-0.1 | Strong regularization |
| **Very limited trials** | 4 | 0.1 | Maximum regularization |

<Tip>
**Pro Tip**: Start with defaults (`dof_offset=2`, `mean_prior_precision=0.01`) and only tune if performance is unsatisfactory. The defaults are optimized for typical BCI scenarios.
</Tip>

<Warning>
**Required**: You must set `predictive_dof_offset` to match `dof_offset`. The SDK enforces this constraint and will raise an `ArgumentError` if they differ. This ensures consistency between training and inference phases.
</Warning>

## Training Requirements

### Data Requirements

- **Minimum**: 40 trials per class (160 total for 4-class)
- **Recommended**: 80+ trials per class (320+ total for 4-class)
- **For calibration**: 10-20 trials per class sufficient

<Warning>
**Bayesian LDA requires at least 2 trials** to estimate class statistics and shared precision matrix. Single-trial training is not statistically valid for LDA and will raise an `ArgumentError`.

Your training data must have shape `(n_features, n_samples, n_trials)` where `n_trials >= 2`.
</Warning>

### Feature Requirements

Bayesian LDA expects **preprocessed features**, not raw EEG:

✅ **Required preprocessing:**
- Bandpass filtering (8-30 Hz for motor imagery)
- Artifact removal (ICA recommended)
- Spatial filtering (CSP for motor imagery)
- Feature extraction (log-variance for CSP features)
- **Temporal aggregation** (handled automatically during training/inference)

❌ **NOT accepted:**
- Raw EEG channels
- Unfiltered data
- Non-extracted features

See [Preprocessing Requirements](/inference-configuration/preprocessing-requirements) for details.

### Temporal Aggregation

<Warning>
**Critical Preprocessing Step**: Before training, the SDK automatically aggregates the temporal dimension of each trial into a single feature vector. This prevents treating temporally correlated samples as independent observations, which would violate the i.i.d. assumption of the model.
</Warning>

The aggregation method depends on your feature type and paradigm:
- **CSP features**: Log-variance aggregation (default for motor imagery)
- **Power spectral features**: Mean or median aggregation
- **Other features**: Configurable via `BCIMetadata.temporal_aggregation`

This aggregation happens automatically during `train_model()` and `predict_batch()` calls.

## Performance Characteristics

### Computational Performance

| Operation | Latency | Notes |
|-----------|---------|-------|
| **Training** | 10-30 seconds | 50 iterations, 100 trials per class |
| **Calibration** | 5-15 seconds | 20 iterations, 20 trials per class |
| **Batch Inference** | 10-20ms per trial | 10 iterations |
| **Streaming Chunk** | 10-20ms | 10 iterations per chunk |

All measurements on standard CPU (no GPU required).

### Classification Accuracy

| Paradigm | Classes | Typical Accuracy | ITR |
|----------|---------|------------------|-----|
| Motor Imagery | 2 (L/R hand) | 75-90% | 15-25 bits/min |
| Motor Imagery | 4 (L/R/Feet/Tongue) | 70-85% | 20-35 bits/min |
| P300 | 2 (Target/Non-target) | 80-95% | 25-40 bits/min |

<Note>
Accuracy is highly subject-dependent. Subject-specific calibration typically improves accuracy by 5-15%.
</Note>

## Model Inspection

### View Model Parameters

```julia
# Model structure
println("Class means:")
for (k, mean) in enumerate(model.means)
    println("  Class $k: ", mean)
end

println("\nShared precision matrix (first 3x3):")
println(model.global_precision[1:3, 1:3])

# Model metadata
println("\nMetadata:")
println("  Name: ", model.metadata.name)
println("  Paradigm: ", model.metadata.paradigm)
println("  Features: ", model.metadata.n_features)
println("  Classes: ", model.metadata.n_classes)
```

### Compare Models

```julia
# Train multiple models and compare
models = []
for n_iter in [20, 50, 100]
    model = train_model(RxLDAModel, train_data; iterations=n_iter)
    results = predict_batch(model, test_data)
    accuracy = sum(results.predictions .== test_labels) / length(test_labels)
    
    println("Iterations: $n_iter, Accuracy: $(round(accuracy*100, digits=1))%")
    push!(models, (n_iter, model, accuracy))
end
```

## Advantages & Limitations

### Advantages

✅ **Fast Training**: Shared covariance estimation is efficient  
✅ **Fast Inference**: Analytical posterior computation (&lt;20ms)  
✅ **Interpretable**: Clear probabilistic formulation  
✅ **Memory Efficient**: Single shared precision matrix  
✅ **Robust**: Handles uncertainty naturally via Bayesian inference  
✅ **Production-Ready**: Battle-tested in real BCI applications  

### Limitations

❌ **Shared Covariance Assumption**: May not fit well if classes have very different spreads  
❌ **Linear Decision Boundary**: Cannot capture non-linear class boundaries  
❌ **Gaussian Assumption**: Assumes normal class distributions  
❌ **Not Ideal for Overlapping Classes**: Use RxGMM for complex distributions  

## Comparison: Bayesian LDA vs Bayesian GMM

| Aspect | Bayesian LDA (RxLDA) | Bayesian GMM (RxGMM) |
|--------|-------|-------|
| **Precision Matrix** | Shared across all classes | Class-specific |
| **Mathematical Model** | Pooled Gaussian Classifier (PGC) | Heteroscedastic Gaussian Classifier (HGC) |
| **Training Speed** | Faster | Slower |
| **Inference Speed** | Faster (~10-15ms) | Slightly slower (~15-20ms) |
| **Flexibility** | Less flexible | More flexible |
| **Best For** | Well-separated classes | Overlapping/complex distributions |
| **Memory** | Lower | Higher |
| **Parameters** | Fewer (n_classes means + 1 precision) | More (n_classes means + precisions) |

**Rule of thumb**: Start with Bayesian LDA. Switch to Bayesian GMM if accuracy is unsatisfactory.

## Next Steps

<CardGroup cols={2}>
  <Card title="Bayesian GMM (RxGMM)" icon="brain" href="/models/rxgmm">
    More flexible Gaussian Mixture Model with class-specific covariances
  </Card>
  <Card title="Training Tutorial" icon="graduation-cap" href="/examples/advanced-applications">
    Complete training walkthrough
  </Card>
  <Card title="Julia SDK" icon="code" href="/api-reference/julia-sdk">
    Full SDK reference
  </Card>
  <Card title="Code Examples" icon="brackets-curly" href="/examples/code-samples">
    Working examples
  </Card>
</CardGroup>

## References

**Implementation:**
- RxInfer.jl: https://rxinfer.com/
- Source code: `/src/models/rxlda/` in NimbusSDK.jl

**Theory:**
- Fisher, R. A. (1936). "The use of multiple measurements in taxonomic problems"
- Bishop, C. M. (2006). "Pattern Recognition and Machine Learning" (Chapter 4)
- Pooled Gaussian Classifier (PGC) with shared covariance structure

**BCI Applications:**
- Blankertz et al. (2008). "Optimizing spatial filters for robust EEG single-trial analysis"
- Lotte et al. (2018). "A review of classification algorithms for EEG-based BCI"

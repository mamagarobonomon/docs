---
title: "POMDP BCI Control"
description: "Partially Observable Markov Decision Processes for decision-making under uncertainty in BCI systems"
icon: "shield"
---

# POMDP Control for BCI Decision Making

Partially Observable Markov Decision Processes (POMDPs) provide a principled framework for BCI systems to make control decisions under uncertainty. Unlike traditional classification approaches, POMDPs explicitly model the uncertainty in neural signal interpretation and make optimal decisions that balance acting quickly versus waiting for more information.

## Problem Overview

BCI systems face fundamental challenges in decision-making:

- **Observation Uncertainty**: Neural signals are noisy and ambiguous
- **Temporal Trade-offs**: Acting quickly vs. waiting for more evidence
- **Cost of Errors**: Wrong actions can have serious consequences
- **User Adaptation**: User intentions and neural patterns change over time

POMDPs address these challenges by:

- **Explicit Uncertainty Modeling**: Maintaining belief states over user intentions
- **Optimal Action Selection**: Choosing actions that maximize expected utility
- **Information Value**: Considering the value of waiting for more information
- **Risk Management**: Incorporating costs of different types of errors

## Mathematical Foundation

### POMDP Components

A POMDP for BCI control consists of:

1. **States (S)**: Hidden user intentions (e.g., move left, right, select, rest)
2. **Actions (A)**: BCI system actions (e.g., execute command, wait, request clarification)
3. **Observations (O)**: Neural signal measurements and features
4. **Transition Model**: How user intentions evolve over time
5. **Observation Model**: How neural signals relate to intentions
6. **Reward Model**: Costs and benefits of different state-action pairs

### Nimbus Implementation

```python
@model
def bci_pomdp_control(neural_observations, actions, rewards, context):
    """
    POMDP model for BCI control under uncertainty
    
    Args:
        neural_observations: Noisy neural signal measurements
        actions: Control actions taken by BCI system
        rewards: User satisfaction/task success feedback
        context: Environmental and task context
    """
    
    # State space: user intentions
    intention_states = ['rest', 'move_left', 'move_right', 'move_up', 'move_down', 'select', 'cancel']
    num_states = len(intention_states)
    
    # Action space: BCI system actions
    bci_actions = ['wait', 'move_left', 'move_right', 'move_up', 'move_down', 'select', 'cancel', 'clarify']
    num_actions = len(bci_actions)
    
    # Context-dependent state transition probabilities
    for context_type in range(num_context_types):
        for s in range(num_states):
            # Intention persistence depends on context
            persistence_factor = context_persistence[context_type]
            
            transition_alpha = np.ones(num_states) * 0.1
            transition_alpha[s] = persistence_factor  # Self-transition bias
            
            intention_transition_probs[context_type, s] ~ Dirichlet(alpha=transition_alpha)
    
    # Observation model: neural signals given intentions
    for s in range(num_states):
        # Each intention has characteristic neural signature
        observation_means[s] ~ MultivariateNormal(
            mean=intention_neural_signatures[s],
            covariance=np.eye(num_neural_features) * 0.5
        )
        
        # Precision varies by intention clarity
        observation_precision[s] ~ Gamma(
            alpha=intention_clarity[s],  # More clear intentions have higher precision
            beta=1.0
        )
    
    # Reward model: user satisfaction given state-action pairs
    for s in range(num_states):
        for a in range(num_actions):
            # Base reward for correct action matching intention
            if intention_action_match[s, a]:
                base_reward = 1.0
            elif a == 0:  # 'wait' action
                base_reward = -0.1  # Small cost for delay
            else:
                base_reward = -1.0  # Penalty for incorrect action
            
            # Context and urgency modulate rewards
            reward_mean[s, a] ~ Normal(
                mean=base_reward * urgency_factor,
                variance=0.2
            )
            
            reward_precision[s, a] ~ Gamma(alpha=2.0, beta=1.0)
    
    # POMDP sequence
    num_timesteps = len(neural_observations)
    
    # Initial belief state
    initial_belief ~ Dirichlet(alpha=np.ones(num_states))
    
    # Initial user intention
    user_intention[0] ~ Categorical(probs=initial_belief)
    
    for t in range(num_timesteps):
        if t > 0:
            # Intention transition (context-dependent)
            current_context = context[t]
            user_intention[t] ~ Categorical(
                probs=intention_transition_probs[current_context, user_intention[t-1]]
            )
        
        # Neural observation given current intention
        neural_observations[t] ~ MultivariateNormal(
            mean=observation_means[user_intention[t]],
            precision=observation_precision[user_intention[t]]
        )
        
        # BCI action selection (policy to be optimized)
        actions[t] ~ Categorical(probs=action_policy[t])
        
        # Reward feedback
        rewards[t] ~ Normal(
            mean=reward_mean[user_intention[t], actions[t]],
            precision=reward_precision[user_intention[t], actions[t]]
        )
```

## Real-Time Implementation

### POMDP BCI Controller

```python
class POMDPBCIController:
    """POMDP-based BCI controller with belief state tracking"""
    
    def __init__(self, intention_states=None, bci_actions=None, confidence_threshold=0.8):
        if intention_states is None:
            self.intention_states = ['rest', 'move_left', 'move_right', 'select', 'cancel']
        else:
            self.intention_states = intention_states
            
        if bci_actions is None:
            self.bci_actions = ['wait', 'move_left', 'move_right', 'select', 'cancel', 'clarify']
        else:
            self.bci_actions = bci_actions
        
        self.num_states = len(self.intention_states)
        self.num_actions = len(self.bci_actions)
        self.confidence_threshold = confidence_threshold
        
        # POMDP parameters (learned during calibration)
        self.transition_probs = None
        self.observation_means = None
        self.observation_covs = None
        self.reward_matrix = None
        
        # Current belief state
        self.belief_state = np.ones(self.num_states) / self.num_states
        
        # Action history for learning
        self.action_history = []
        self.reward_history = []
        self.observation_history = []
        
        # Performance tracking
        self.decision_times = []
        self.accuracy_history = []
        
    def calibrate_pomdp(self, training_data, training_labels, training_rewards):
        """Calibrate POMDP parameters from training data"""
        
        # Learn observation model
        self.learn_observation_model(training_data, training_labels)
        
        # Learn transition model
        self.learn_transition_model(training_labels)
        
        # Learn reward model
        self.learn_reward_model(training_labels, training_rewards)
        
        # Validate calibration
        validation_accuracy = self.validate_pomdp_model(training_data, training_labels)
        
        return {
            'calibration_success': True,
            'validation_accuracy': validation_accuracy,
            'num_training_samples': len(training_data),
            'model_parameters': {
                'transition_probs': self.transition_probs.shape,
                'observation_means': self.observation_means.shape,
                'reward_matrix': self.reward_matrix.shape
            }
        }
    
    def learn_observation_model(self, neural_data, labels):
        """Learn neural observation model from training data"""
        
        self.observation_means = np.zeros((self.num_states, neural_data.shape[1]))
        self.observation_covs = np.zeros((self.num_states, neural_data.shape[1], neural_data.shape[1]))
        
        for state in range(self.num_states):
            state_data = neural_data[labels == state]
            
            if len(state_data) > 0:
                self.observation_means[state] = np.mean(state_data, axis=0)
                self.observation_covs[state] = np.cov(state_data.T)
                
                # Regularize covariance matrix
                self.observation_covs[state] += np.eye(neural_data.shape[1]) * 0.01
            else:
                # Default parameters if no training data for this state
                self.observation_means[state] = np.mean(neural_data, axis=0)
                self.observation_covs[state] = np.eye(neural_data.shape[1])
    
    def learn_transition_model(self, label_sequence):
        """Learn state transition probabilities"""
        
        self.transition_probs = np.zeros((self.num_states, self.num_states))
        
        # Count transitions
        for t in range(1, len(label_sequence)):
            prev_state = label_sequence[t-1]
            curr_state = label_sequence[t]
            self.transition_probs[prev_state, curr_state] += 1
        
        # Normalize and add smoothing
        for s in range(self.num_states):
            if np.sum(self.transition_probs[s]) > 0:
                self.transition_probs[s] /= np.sum(self.transition_probs[s])
            else:
                self.transition_probs[s] = np.ones(self.num_states) / self.num_states
            
            # Add Dirichlet smoothing
            self.transition_probs[s] = (self.transition_probs[s] + 0.1) / (1 + 0.1 * self.num_states)
    
    def learn_reward_model(self, labels, rewards):
        """Learn reward model from user feedback"""
        
        self.reward_matrix = np.zeros((self.num_states, self.num_actions))
        reward_counts = np.zeros((self.num_states, self.num_actions))
        
        # This is simplified - in practice, would need action labels too
        for state in range(self.num_states):
            state_rewards = rewards[labels == state]
            if len(state_rewards) > 0:
                # Assume uniform action distribution for simplicity
                for action in range(self.num_actions):
                    self.reward_matrix[state, action] = np.mean(state_rewards)
        
        # Set default reward structure
        for s in range(self.num_states):
            for a in range(self.num_actions):
                if self.reward_matrix[s, a] == 0:  # No training data
                    if s == a:  # Correct action
                        self.reward_matrix[s, a] = 1.0
                    elif a == 0:  # Wait action
                        self.reward_matrix[s, a] = -0.1
                    else:  # Incorrect action
                        self.reward_matrix[s, a] = -1.0
    
    def update_belief_state(self, neural_observation, last_action=None):
        """Update belief state using Bayesian filtering"""
        
        # Prediction step (if action was taken)
        if last_action is not None:
            # Transition probabilities might depend on actions (simplified here)
            predicted_belief = self.transition_probs.T @ self.belief_state
        else:
            predicted_belief = self.belief_state
        
        # Update step using observation
        observation_likelihoods = np.zeros(self.num_states)
        
        for state in range(self.num_states):
            # Compute likelihood of observation given state
            mean = self.observation_means[state]
            cov = self.observation_covs[state]
            
            likelihood = multivariate_normal.pdf(neural_observation, mean, cov)
            observation_likelihoods[state] = likelihood
        
        # Posterior belief (Bayes rule)
        posterior_belief = predicted_belief * observation_likelihoods
        
        # Normalize
        if np.sum(posterior_belief) > 0:
            posterior_belief /= np.sum(posterior_belief)
        else:
            # Fallback to uniform if all likelihoods are zero
            posterior_belief = np.ones(self.num_states) / self.num_states
        
        self.belief_state = posterior_belief
        
        return {
            'belief_state': posterior_belief,
            'most_likely_state': np.argmax(posterior_belief),
            'confidence': np.max(posterior_belief),
            'entropy': -np.sum(posterior_belief * np.log(posterior_belief + 1e-10))
        }
    
    def select_optimal_action(self, neural_observation, context=None):
        """Select optimal action using POMDP policy"""
        
        start_time = time.perf_counter()
        
        # Update belief state
        belief_info = self.update_belief_state(neural_observation)
        
        # Compute expected value for each action
        action_values = np.zeros(self.num_actions)
        
        for action in range(self.num_actions):
            expected_reward = 0.0
            
            for state in range(self.num_states):
                # Immediate reward
                immediate_reward = self.reward_matrix[state, action]
                
                # Future value (simplified - could use full POMDP solver)
                future_value = self.estimate_future_value(state, action, context)
                
                # Discount factor
                gamma = 0.9
                
                total_value = immediate_reward + gamma * future_value
                expected_reward += self.belief_state[state] * total_value
            
            action_values[action] = expected_reward
        
        # Select best action
        best_action = np.argmax(action_values)
        best_value = action_values[best_action]
        
        # Conservative policy: only act if confident enough
        confidence = belief_info['confidence']
        
        if confidence < self.confidence_threshold and best_action != 0:  # 0 = 'wait'
            selected_action = 0  # Wait for more information
            decision_rationale = 'insufficient_confidence'
        else:
            selected_action = best_action
            decision_rationale = 'confident_decision'
        
        decision_time = (time.perf_counter() - start_time) * 1000
        self.decision_times.append(decision_time)
        
        return {
            'action': self.bci_actions[selected_action],
            'action_index': selected_action,
            'expected_value': action_values[selected_action],
            'confidence': confidence,
            'belief_state': self.belief_state.copy(),
            'decision_rationale': decision_rationale,
            'decision_time_ms': decision_time,
            'all_action_values': action_values
        }
    
    def estimate_future_value(self, state, action, context):
        """Estimate future value (simplified value function)"""
        
        # This is a simplified heuristic - full POMDP would use value iteration
        if action == 0:  # Wait action
            # Value of waiting depends on current uncertainty
            uncertainty = 1.0 - np.max(self.belief_state)
            return 0.5 * uncertainty  # Waiting is more valuable when uncertain
        else:
            # Value of acting depends on correctness probability
            return self.belief_state[state] * 0.8  # Discounted future reward
    
    def learn_from_feedback(self, neural_obs, action_taken, user_feedback):
        """Online learning from user feedback"""
        
        # Store experience
        self.observation_history.append(neural_obs)
        self.action_history.append(action_taken)
        self.reward_history.append(user_feedback['satisfaction'])
        
        # Update reward model based on feedback
        if 'true_intention' in user_feedback:
            true_state = self.intention_states.index(user_feedback['true_intention'])
            
            # Update reward for this state-action pair
            learning_rate = 0.1
            current_reward = self.reward_matrix[true_state, action_taken]
            new_reward = user_feedback['satisfaction']
            
            self.reward_matrix[true_state, action_taken] = \
                (1 - learning_rate) * current_reward + learning_rate * new_reward
        
        # Update observation model if needed
        if user_feedback['satisfaction'] < 0.3:  # Poor performance
            self.adapt_observation_model(neural_obs, user_feedback)
    
    def adapt_observation_model(self, neural_obs, feedback):
        """Adapt observation model based on poor performance"""
        
        if 'true_intention' in feedback:
            true_state = self.intention_states.index(feedback['true_intention'])
            
            # Move observation mean slightly toward this observation
            adaptation_rate = 0.05
            self.observation_means[true_state] = \
                (1 - adaptation_rate) * self.observation_means[true_state] + \
                adaptation_rate * neural_obs

# Usage example for prosthetic hand control
class ProstheticHandPOMDP(POMDPBCIController):
    """POMDP controller for prosthetic hand"""
    
    def __init__(self):
        super().__init__(
            intention_states=['rest', 'open_hand', 'close_hand', 'pinch', 'point'],
            bci_actions=['wait', 'open_hand', 'close_hand', 'pinch', 'point', 'stop'],
            confidence_threshold=0.75  # Higher threshold for safety
        )
        
        # Prosthetic-specific parameters
        self.safety_timeout = 5.0  # seconds
        self.last_action_time = 0
        
    def safe_action_selection(self, neural_observation):
        """Action selection with safety constraints"""
        
        decision = self.select_optimal_action(neural_observation)
        
        # Safety checks
        current_time = time.time()
        time_since_last_action = current_time - self.last_action_time
        
        # Prevent rapid successive actions
        if decision['action'] != 'wait' and time_since_last_action < 1.0:
            return {
                'action': 'wait',
                'rationale': 'safety_timeout',
                'original_decision': decision
            }
        
        # Extra confirmation for potentially dangerous actions
        if decision['action'] in ['close_hand', 'pinch'] and decision['confidence'] < 0.9:
            return {
                'action': 'wait',
                'rationale': 'insufficient_confidence_for_grip',
                'original_decision': decision
            }
        
        if decision['action'] != 'wait':
            self.last_action_time = current_time
        
        return decision

# Prosthetic hand BCI system
prosthetic_bci = ProstheticHandPOMDP()

# Calibration
calibration_result = prosthetic_bci.calibrate_pomdp(
    training_neural_data, training_labels, training_rewards
)

print(f"POMDP calibrated with {calibration_result['validation_accuracy']:.2f} accuracy")

# Real-time control with safety
for neural_signal in emg_eeg_stream():
    # Extract features
    neural_features = extract_prosthetic_features(neural_signal)
    
    # POMDP decision making
    decision = prosthetic_bci.safe_action_selection(neural_features)
    
    if decision['action'] != 'wait':
        print(f"Executing: {decision['action']} "
              f"(confidence: {decision['confidence']:.2f})")
        
        # Execute prosthetic command
        execute_prosthetic_action(decision['action'])
        
        # Monitor user satisfaction
        feedback = monitor_user_satisfaction()
        prosthetic_bci.learn_from_feedback(
            neural_features, decision['action_index'], feedback
        )
    else:
        print(f"Waiting... ({decision.get('rationale', 'low_confidence')})")
```

## Advanced Applications

### Adaptive BCI Interface

```python
class AdaptiveBCIInterface(POMDPBCIController):
    """POMDP-based adaptive BCI interface"""
    
    def __init__(self):
        super().__init__(
            intention_states=['navigate', 'select', 'scroll_up', 'scroll_down', 'back', 'menu'],
            bci_actions=['wait', 'navigate', 'select', 'scroll_up', 'scroll_down', 'back', 'menu', 'clarify']
        )
        
        # Interface adaptation parameters
        self.difficulty_level = 0.5  # 0 = easy, 1 = hard
        self.user_performance_history = []
        
    def adapt_interface_difficulty(self, recent_performance):
        """Adapt interface difficulty based on user performance"""
        
        if recent_performance > 0.8:
            # User performing well - can increase difficulty
            self.difficulty_level = min(1.0, self.difficulty_level + 0.1)
            self.confidence_threshold = max(0.6, self.confidence_threshold - 0.05)
        elif recent_performance < 0.5:
            # User struggling - decrease difficulty
            self.difficulty_level = max(0.0, self.difficulty_level - 0.1)
            self.confidence_threshold = min(0.9, self.confidence_threshold + 0.05)
        
        return {
            'new_difficulty': self.difficulty_level,
            'new_threshold': self.confidence_threshold,
            'adaptation_reason': 'performance_based'
        }
    
    def context_aware_decision(self, neural_obs, interface_context):
        """Make decisions considering interface context"""
        
        # Modify action values based on context
        base_decision = self.select_optimal_action(neural_obs, interface_context)
        
        # Context-specific adjustments
        if interface_context.get('critical_action', False):
            # Require higher confidence for critical actions
            if base_decision['confidence'] < 0.9 and base_decision['action'] != 'wait':
                return {
                    'action': 'clarify',
                    'rationale': 'critical_action_confirmation',
                    'original_decision': base_decision
                }
        
        if interface_context.get('time_pressure', False):
            # Lower threshold under time pressure
            if base_decision['action'] == 'wait' and base_decision['confidence'] > 0.5:
                # Override wait decision
                second_best = np.argsort(base_decision['all_action_values'])[-2]
                return {
                    'action': self.bci_actions[second_best],
                    'rationale': 'time_pressure_override',
                    'confidence': base_decision['confidence']
                }
        
        return base_decision

# Adaptive interface usage
adaptive_interface = AdaptiveBCIInterface()

# Calibration
adaptive_interface.calibrate_pomdp(training_data, labels, rewards)

# Real-time adaptive control
performance_window = []

for neural_signal in bci_stream():
    # Extract features
    features = extract_interface_features(neural_signal)
    
    # Get interface context
    context = get_interface_context()
    
    # Context-aware decision
    decision = adaptive_interface.context_aware_decision(features, context)
    
    if decision['action'] != 'wait':
        # Execute interface action
        success = execute_interface_action(decision['action'])
        performance_window.append(success)
        
        # Adapt interface every 20 actions
        if len(performance_window) >= 20:
            recent_performance = np.mean(performance_window)
            adaptation = adaptive_interface.adapt_interface_difficulty(recent_performance)
            
            print(f"Interface adapted: difficulty={adaptation['new_difficulty']:.2f}")
            performance_window = []
```

## Performance Characteristics

### Decision Quality

<CardGroup cols={2}>
  <Card title="Accuracy vs. Speed" icon="balance-scale">
    **Conservative Policy**: High accuracy (90-95%) with moderate speed
    
    **Aggressive Policy**: Higher speed with acceptable accuracy (80-85%)
    
    **Adaptive Threshold**: Balances based on context and user performance
  </Card>
  
  <Card title="Safety & Reliability" icon="shield">
    **Error Prevention**: Explicit modeling of action costs
    
    **Graceful Degradation**: Maintains function with uncertain inputs
    
    **User Adaptation**: Learns from feedback to improve over time
  </Card>
</CardGroup>

### Computational Requirements

| Component | Complexity | Real-Time Performance |
|-----------|------------|----------------------|
| Belief Update | O(S²) | Sub-10ms for S≤10 states |
| Action Selection | O(SA) | Sub-5ms for typical problems |
| Value Estimation | O(S²A) | Sub-20ms with approximation |
| Online Learning | O(S) | Sub-2ms per feedback |

## Clinical Applications

### Safety-Critical BCI Systems

For medical applications, POMDPs provide essential safety guarantees:

- **Surgical Robotics**: High confidence thresholds prevent dangerous actions
- **Life Support Control**: Conservative policies prioritize patient safety
- **Prosthetic Control**: Prevents unintended harmful movements
- **Wheelchair Navigation**: Collision avoidance through uncertainty modeling

## Next Steps

<CardGroup cols={2}>
  <Card title="Sensor Fusion" icon="layers" href="/models/sensor-fusion">
    Combine POMDP control with multi-modal sensing
  </Card>
  <Card title="Kalman Filtering" icon="wave-square" href="/models/kalman-eeg-filtering">
    Preprocessing for POMDP observation models
  </Card>
  <Card title="Implementation" icon="code" href="/examples/code-samples">
    Complete POMDP implementation examples
  </Card>
  <Card title="Safety Applications" icon="heartbeat" href="/examples/industry-use-cases">
    Medical and safety-critical BCI deployments
  </Card>
</CardGroup>

POMDP control represents the state-of-the-art in intelligent BCI decision-making, providing principled uncertainty management and optimal action selection for safe, reliable brain-computer interfaces.

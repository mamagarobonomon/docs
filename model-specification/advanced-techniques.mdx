---
title: "Advanced Modeling Techniques"
description: "Advanced usage patterns for Nimbus BCI models"
icon: "sparkles"
---

# Advanced Modeling Techniques

This page covers advanced techniques for optimizing Nimbus BCI models beyond basic usage. These techniques help you achieve better performance, robustness, and generalization.

<Info>
**Prerequisites**: Familiarity with basic model training and inference. See [Python SDK Quickstart](/python-sdk/quickstart) or [Julia SDK Quickstart](/quickstart) first.
</Info>

---

## 1. Hyperparameter Optimization

### Grid Search with Cross-Validation

Systematically search for optimal hyperparameters:

<Tabs>
  <Tab title="Python">

```python
from sklearn.model_selection import GridSearchCV
from nimbus_pysdk import NimbusLDA
import numpy as np

# Define parameter grid
param_grid = {
    'mu_scale': [1.0, 3.0, 5.0, 7.0, 10.0],
    'class_prior_alpha': [0.5, 1.0, 2.0, 5.0]
}

# Grid search with 5-fold CV
grid = GridSearchCV(
    NimbusLDA(),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=2
)

grid.fit(X_train, y_train)

print(f"Best parameters: {grid.best_params_}")
print(f"Best CV score: {grid.best_score_:.2%}")

# Use best model
best_clf = grid.best_estimator_
test_score = best_clf.score(X_test, y_test)
print(f"Test accuracy: {test_score:.2%}")
```
</Tab>
  <Tab title="Julia">

```julia
using NimbusSDK

# Define hyperparameter grid
mu_scales = [1.0, 3.0, 5.0, 7.0, 10.0]
dof_offsets = [2, 5, 10, 15]

best_accuracy = 0.0
best_params = nothing

# Grid search
for mu_scale in mu_scales
    for dof_offset in dof_offsets
        # Train with these hyperparameters
        model = train_model(
            RxLDAModel,
            train_data;
            iterations = 50,
            mu_scale = mu_scale,
            dof_offset = dof_offset,
            showprogress = false
        )
        
        # Validate
        results = predict_batch(model, val_data)
        accuracy = sum(results.predictions .== val_data.labels) / length(val_data.labels)
        
        if accuracy > best_accuracy
            best_accuracy = accuracy
            best_params = (mu_scale=mu_scale, dof_offset=dof_offset)
        end
    end
end

println("Best parameters: $best_params")
println("Best accuracy: $(round(best_accuracy*100, digits=1))%")
```
</Tab>
</Tabs>

### Bayesian Optimization

For expensive hyperparameter searches, use Bayesian optimization:

```python
from skopt import BayesSearchCV
from nimbus_pysdk import NimbusGMM

# Define search space
search_space = {
    'mu_scale': (0.1, 10.0, 'log-uniform'),
    'class_prior_alpha': (0.1, 5.0, 'log-uniform')
}

# Bayesian optimization
bayes_search = BayesSearchCV(
    NimbusGMM(),
    search_space,
    n_iter=30,
    cv=5,
    n_jobs=-1,
    verbose=2
)

bayes_search.fit(X_train, y_train)
```

---

## 2. Cross-Subject Transfer Learning

### Multi-Subject Training

Train on data from multiple subjects for better generalization:

<Tabs>
  <Tab title="Python">

```python
from nimbus_pysdk import NimbusLDA, estimate_normalization_params, apply_normalization
import numpy as np

# Collect data from multiple subjects
subjects = ['S01', 'S02', 'S03', 'S04', 'S05']
all_X = []
all_y = []

for subject_id in subjects:
    X_subj, y_subj = load_subject_data(subject_id)
    all_X.append(X_subj)
    all_y.append(y_subj)

# Combine data
X_combined = np.vstack(all_X)
y_combined = np.concatenate(all_y)

# Critical: Normalize across subjects
norm_params = estimate_normalization_params(X_combined, method="zscore")
X_norm = apply_normalization(X_combined, norm_params)

# Train with higher regularization for generalization
clf = NimbusLDA(mu_scale=7.0)  # Higher than single-subject
clf.fit(X_norm, y_combined)

# For new subject: normalize with same parameters, then calibrate
X_new_subject = load_new_subject_data()
X_new_norm = apply_normalization(X_new_subject, norm_params)
clf.partial_fit(X_new_norm[:20], y_new[:20])  # 20-trial calibration
```
</Tab>
  <Tab title="Julia">

```julia
using NimbusSDK

# Load data from multiple subjects
subjects = ["S01", "S02", "S03", "S04", "S05"]
all_features = []
all_labels = []

for subject_id in subjects
    features, labels = load_subject_data(subject_id)
    push!(all_features, features)
    push!(all_labels, labels)
end

# Combine and normalize
combined_features = cat(all_features..., dims=3)
combined_labels = vcat(all_labels...)

# Train cross-subject model
cross_subject_model = train_model(
    RxLDAModel,
    BCIData(combined_features, metadata, combined_labels);
    iterations = 50,
    mu_scale = 7.0  # Higher regularization
)

# Calibrate for new subject
new_subject_data = load_new_subject_data()
personalized_model = calibrate_model(
    cross_subject_model,
    new_subject_data;
    iterations = 20
)
```
</Tab>
</Tabs>

### Domain Adaptation

Adapt models trained on one dataset to another:

```python
from nimbus_pysdk import NimbusLDA
import numpy as np

# Train on source domain (e.g., lab data)
clf_source = NimbusLDA(mu_scale=3.0)
clf_source.fit(X_source, y_source)

# Adapt to target domain (e.g., home use)
# Use unlabeled target data for normalization
norm_params_target = estimate_normalization_params(X_target_unlabeled, method="zscore")
X_target_norm = apply_normalization(X_target, norm_params_target)

# Fine-tune with small amount of labeled target data
clf_source.partial_fit(X_target_norm[:30], y_target[:30])
```

---

## 3. Ensemble Methods

### Model Averaging

Combine predictions from multiple models:

```python
from nimbus_pysdk import NimbusLDA, NimbusGMM, NimbusSoftmax
import numpy as np

# Train multiple models
clf_lda = NimbusLDA()
clf_lda.fit(X_train, y_train)

clf_gmm = NimbusGMM()
clf_gmm.fit(X_train, y_train)

clf_softmax = NimbusSoftmax()
clf_softmax.fit(X_train, y_train)

# Ensemble prediction (average probabilities)
probs_lda = clf_lda.predict_proba(X_test)
probs_gmm = clf_gmm.predict_proba(X_test)
probs_softmax = clf_softmax.predict_proba(X_test)

# Weighted average (tune weights on validation set)
weights = [0.4, 0.4, 0.2]  # LDA, GMM, Softmax
ensemble_probs = (
    weights[0] * probs_lda +
    weights[1] * probs_gmm +
    weights[2] * probs_softmax
)

ensemble_predictions = ensemble_probs.argmax(axis=1)
```

### Bootstrap Aggregating (Bagging)

Train multiple models on bootstrap samples:

```python
from nimbus_pysdk import NimbusLDA
import numpy as np

n_models = 10
models = []

for i in range(n_models):
    # Bootstrap sample
    indices = np.random.choice(len(X_train), len(X_train), replace=True)
    X_boot = X_train[indices]
    y_boot = y_train[indices]
    
    # Train model
    clf = NimbusLDA()
    clf.fit(X_boot, y_boot)
    models.append(clf)

# Aggregate predictions
all_probs = np.array([clf.predict_proba(X_test) for clf in models])
bagged_probs = all_probs.mean(axis=0)
bagged_predictions = bagged_probs.argmax(axis=1)
```

---

## 4. Active Learning

Select most informative samples for labeling:

```python
from nimbus_pysdk import NimbusLDA, compute_entropy
import numpy as np

# Initial training on small labeled set
clf = NimbusLDA()
clf.fit(X_labeled, y_labeled)

# Pool of unlabeled data
X_unlabeled = load_unlabeled_data()

# Active learning loop
n_iterations = 10
samples_per_iteration = 5

for iteration in range(n_iterations):
    # Predict on unlabeled data
    probs = clf.predict_proba(X_unlabeled)
    
    # Select most uncertain samples (highest entropy)
    entropy = compute_entropy(probs)
    uncertain_indices = entropy.argsort()[-samples_per_iteration:]
    
    # Query labels for these samples (from user or oracle)
    X_to_label = X_unlabeled[uncertain_indices]
    y_new = query_labels(X_to_label)  # Your labeling function
    
    # Update model
    clf.partial_fit(X_to_label, y_new)
    
    # Remove from unlabeled pool
    X_unlabeled = np.delete(X_unlabeled, uncertain_indices, axis=0)
    
    print(f"Iteration {iteration+1}: Added {samples_per_iteration} samples")
```

---

## 5. Confidence Calibration

Ensure predicted probabilities match actual accuracy:

```python
from nimbus_pysdk import NimbusLDA, compute_calibration_metrics
from sklearn.calibration import CalibratedClassifierCV
import numpy as np

# Train base model
clf = NimbusLDA()
clf.fit(X_train, y_train)

# Check calibration
probs = clf.predict_proba(X_val)
preds = probs.argmax(axis=1)
confidences = probs.max(axis=1)

calib_metrics = compute_calibration_metrics(preds, confidences, y_val)
print(f"ECE (before): {calib_metrics.ece:.3f}")
print(f"MCE (before): {calib_metrics.mce:.3f}")

# Calibrate probabilities
clf_calibrated = CalibratedClassifierCV(clf, method='isotonic', cv='prefit')
clf_calibrated.fit(X_val, y_val)

# Check calibration after
probs_cal = clf_calibrated.predict_proba(X_test)
preds_cal = probs_cal.argmax(axis=1)
confidences_cal = probs_cal.max(axis=1)

calib_metrics_after = compute_calibration_metrics(preds_cal, confidences_cal, y_test)
print(f"ECE (after): {calib_metrics_after.ece:.3f}")
print(f"MCE (after): {calib_metrics_after.mce:.3f}")
```

---

## 6. Feature Selection & Dimensionality Reduction

### Recursive Feature Elimination

```python
from sklearn.feature_selection import RFE
from nimbus_pysdk import NimbusLDA

# Recursive feature elimination
clf = NimbusLDA()
rfe = RFE(estimator=clf, n_features_to_select=8, step=1)
rfe.fit(X_train, y_train)

# Transform data
X_train_selected = rfe.transform(X_train)
X_test_selected = rfe.transform(X_test)

# Train on selected features
clf_final = NimbusLDA()
clf_final.fit(X_train_selected, y_train)
```

### Fisher Score Feature Ranking

```python
from nimbus_pysdk import compute_fisher_score, rank_features_by_discriminability

# Compute Fisher scores for all features
fisher_scores = compute_fisher_score(X_train, y_train)

# Rank features
ranked_features = rank_features_by_discriminability(X_train, y_train)

# Select top K features
K = 10
top_features = ranked_features[:K]
X_train_top = X_train[:, top_features]
X_test_top = X_test[:, top_features]
```

---

## 7. Dealing with Class Imbalance

### Class Weighting

```python
from nimbus_pysdk import NimbusLDA
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Compute class weights
class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(y_train),
    y=y_train
)

# Apply weights during training (via sample weights)
sample_weights = np.array([class_weights[y] for y in y_train])

# Train with weighted samples
clf = NimbusLDA()
clf.fit(X_train, y_train, sample_weight=sample_weights)
```

### SMOTE (Synthetic Minority Over-sampling)

```python
from imblearn.over_sampling import SMOTE
from nimbus_pysdk import NimbusLDA

# Apply SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Train on balanced data
clf = NimbusLDA()
clf.fit(X_resampled, y_resampled)
```

---

## 8. Model Diagnostics & Debugging

### Learning Curves

```python
from sklearn.model_selection import learning_curve
from nimbus_pysdk import NimbusLDA
import matplotlib.pyplot as plt

# Generate learning curves
train_sizes, train_scores, val_scores = learning_curve(
    NimbusLDA(),
    X_train,
    y_train,
    cv=5,
    train_sizes=np.linspace(0.1, 1.0, 10),
    n_jobs=-1
)

# Plot
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_scores.mean(axis=1), label='Training score')
plt.plot(train_sizes, val_scores.mean(axis=1), label='Validation score')
plt.xlabel('Training examples')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Learning Curves')
plt.show()
```

### Confusion Matrix Analysis

```python
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from nimbus_pysdk import NimbusLDA

clf = NimbusLDA()
clf.fit(X_train, y_train)

predictions = clf.predict(X_test)
cm = confusion_matrix(y_test, predictions)

# Visualize
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.title('Confusion Matrix')
plt.show()
```

---

## Troubleshooting Guide

<AccordionGroup>
  <Accordion title="Model Overfitting" icon="triangle-exclamation">
    **Symptoms**: High training accuracy, low test accuracy
    
    **Solutions**:
    - Increase `mu_scale` (stronger regularization)
    - Use cross-validation for hyperparameter tuning
    - Collect more training data
    - Apply feature selection
    - Use ensemble methods (bagging)
  </Accordion>
  
  <Accordion title="Poor Calibration" icon="gauge">
    **Symptoms**: Predicted probabilities don't match actual accuracy
    
    **Solutions**:
    - Use `CalibratedClassifierCV` from sklearn
    - Collect more diverse training data
    - Check for distribution shift between train and test
  </Accordion>
  
  <Accordion title="Class Imbalance" icon="scale-unbalanced">
    **Symptoms**: Model biased toward majority class
    
    **Solutions**:
    - Use class weighting
    - Apply SMOTE or other resampling techniques
    - Adjust decision threshold based on class priors
    - Use stratified cross-validation
  </Accordion>
  
  <Accordion title="Poor Cross-Subject Generalization" icon="users">
    **Symptoms**: Good within-subject, poor across-subject performance
    
    **Solutions**:
    - Train on multi-subject data
    - Increase regularization (`mu_scale`)
    - Normalize features consistently
    - Use subject-specific calibration
    - Apply domain adaptation techniques
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="Model Specification" icon="cube" href="/model-specification">
    Probabilistic model architecture
  </Card>
  <Card title="BCI Paradigm Examples" icon="beaker" href="/model-specification/bci-examples">
    Motor Imagery, P300, SSVEP
  </Card>
  <Card title="Python SDK" icon="python" href="/python-sdk/introduction">
    sklearn-compatible classifiers
  </Card>
  <Card title="Julia SDK" icon="code" href="/api-reference/julia-sdk">
    RxInfer.jl-based inference
  </Card>
</CardGroup>

---

<Note>
**Purpose of This Page**: This page provides **advanced optimization techniques** for Nimbus BCI models. For basic usage, see [Python SDK Quickstart](/python-sdk/quickstart) or [Julia SDK Quickstart](/quickstart).
</Note>

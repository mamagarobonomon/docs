---
title: "Streaming Inference"
description: "Real-time chunk-by-chunk BCI processing with NimbusSDK.jl"
icon: "wave-square"
---

# Streaming Inference

NimbusSDK.jl provides **client-side streaming inference** for real-time BCI applications. Process EEG data chunk-by-chunk with automatic aggregation and quality assessment.

<Note>
Streaming inference runs **locally on your machine** using the Julia SDK. This is different from a server-based WebSocket API - all processing happens client-side for maximum performance and privacy.
</Note>

## Overview

Streaming inference is ideal for:
- **Real-time BCI control** (cursor movement, prosthetics)
- **Live neurofeedback** applications
- **Continuous monitoring** systems
- **Interactive BCI** experiences

### Key Features

- **Chunk-by-chunk processing**: Process data as it arrives
- **Weighted aggregation**: Automatic voting across chunks
- **Quality assessment**: Real-time confidence monitoring
- **Low latency**: <10ms per chunk processing
- **Local processing**: No network calls required

## Basic Streaming Example

```julia
using NimbusSDK

# Step 1: Authenticate and load model
NimbusSDK.authenticate("your-api-key")
model = load_model(RxLDAModel, "motor_imagery_4class_v1")

# Step 2: Initialize streaming session
# Define chunk size: 250 samples = 1 second at 250 Hz
metadata = BCIMetadata(
    sampling_rate = 250.0,
    paradigm = :motor_imagery,
    feature_type = :csp,
    n_features = 16,
    n_classes = 4,
    chunk_size = 250  # CRITICAL: Set chunk size for streaming
)

session = init_streaming(model, metadata)

# Step 3: Process chunks as they arrive
for chunk in eeg_stream()
    # chunk is Array{Float64, 2} with shape (n_features √ó chunk_size)
    result = process_chunk(session, chunk)
    
    println("Chunk prediction: $(result.prediction) " *
            "(confidence: $(round(result.confidence, digits=3)))")
end

# Step 4: Finalize trial with weighted voting
final_result = finalize_trial(session; method=:weighted_vote)
println("‚úì Final prediction: $(final_result.prediction)")
```

## Complete Motor Imagery Streaming Pipeline

```julia
using NimbusSDK

# Initialize
NimbusSDK.authenticate("your-api-key")
model = load_model(RxLDAModel, "motor_imagery_4class_v1")

metadata = BCIMetadata(
    sampling_rate = 250.0,
    paradigm = :motor_imagery,
    feature_type = :csp,
    n_features = 16,
    n_classes = 4,
    chunk_size = 250  # 1 second chunks
)

session = init_streaming(model, metadata)

# Simulate real-time EEG stream
function simulate_eeg_stream()
    # In practice, this would come from your EEG hardware
    for trial in 1:20
        for chunk_idx in 1:4  # 4 seconds total, 4 chunks
            chunk = randn(16, 250)  # 16 features √ó 250 samples
            yield chunk
        end
    end
end

# Process streaming data
for chunk in simulate_eeg_stream()
    result = process_chunk(session, chunk)
    
    # Implement your real-time control logic here
    if result.prediction == 1 && result.confidence > 0.7
        println("üîÑ Moving cursor left (confidence: $(result.confidence))")
    elseif result.prediction == 2 && result.confidence > 0.7
        println("üîÑ Moving cursor right (confidence: $(result.confidence))")
    end
end

# Get final trial result
final = finalize_trial(session; method=:weighted_vote)
println("\n‚úì Trial complete: Class $(final.prediction), Confidence: $(final.confidence)")
```

## Aggregation Methods

Streaming uses multiple chunks per trial and aggregates them into a final prediction.

### Weighted Vote (Recommended)

Weights chunks by confidence:

```julia
final_result = finalize_trial(session; method=:weighted_vote)
```

**How it works:**
- Each chunk gets a vote weighted by its confidence
- Class with highest weighted vote wins
- More reliable than majority vote

### Majority Vote

Simple majority across all chunks:

```julia
final_result = finalize_trial(session; method=:majority_vote)
```

### Example: 4-Chunk Trial

```julia
# Chunk 1: Class 1, confidence 0.7
# Chunk 2: Class 1, confidence 0.8
# Chunk 3: Class 2, confidence 0.6
# Chunk 4: Class 1, confidence 0.9

# Weighted vote: Class 1 (0.7 + 0.8 + 0.9 = 2.4) > Class 2 (0.6)
# Majority vote: Class 1 (3 votes) > Class 2 (1 vote)
```

## Quality Assessment

Monitor trial quality in real-time:

```julia
results = process_chunk(session, chunk)

# Check if chunk should be rejected
if should_reject_trial(results.confidence, 0.7)
    println("‚ö†Ô∏è Low confidence chunk - consider rejection")
end

# Get final quality assessment
final = finalize_trial(session)
quality = assess_trial_quality([final])

if !quality.confidence_acceptable
    println("‚ö†Ô∏è Final trial quality insufficient")
end
```

## Streaming Session Lifecycle

### 1. Initialize Session

```julia
session = init_streaming(model, metadata)
# Session is ready to process chunks
```

### 2. Process Chunks

```julia
for chunk in data_stream
    result = process_chunk(session, chunk)
    # Handle real-time predictions
end
```

### 3. Finalize Trial

```julia
final = finalize_trial(session; method=:weighted_vote)
# Get aggregate prediction for the entire trial
```

### 4. Reset for Next Trial

```julia
# Start a new trial with the same session
# (session automatically resets after finalize_trial)
```

## Configuration

### Chunk Size

Set `chunk_size` in metadata to enable streaming:

```julia
metadata = BCIMetadata(
    # ... other fields ...
    chunk_size = 250  # Samples per chunk
)
```

**Recommendations:**
- **Motor Imagery**: 250-500 samples (1-2 seconds at 250 Hz)
- **P300**: 200-250 samples (0.8-1 seconds at 250 Hz)
- **SSVEP**: 500-1000 samples (2-4 seconds at 250 Hz)

### Number of Chunks per Trial

Control how many chunks aggregate into one trial:

```julia
# Example: 4-second motor imagery trial
# If chunk_size = 250 (1 second), then:
# - 4 chunks total
# - Trial duration = 4 seconds

# Process all chunks
for chunk in 1:4
    result = process_chunk(session, get_chunk(chunk))
end

# Get final result
final = finalize_trial(session)
```

## Performance

### Latency

| Operation | Typical Latency |
|-----------|-----------------|
| Single chunk processing | 5-10 ms |
| Aggregation (4 chunks) | <1 ms |
| Total trial latency | 20-40 ms |

### Throughput

- **At 250 Hz**: Process one chunk every second
- **4 chunks per trial**: ~4 second trials
- **Overhead**: Minimal (<1 ms per chunk)

## Real-Time BCI Control Example

```julia
using NimbusSDK

# Setup streaming
model = load_model(RxLDAModel, "motor_imagery_4class_v1")
session = init_streaming(model, metadata)

# Real-time cursor control
function handle_cursor_control(result)
    pred = result.prediction
    conf = result.confidence
    
    if conf > 0.7
        if pred == 1
            move_cursor_left()
        elseif pred == 2
            move_cursor_right()
        elseif pred == 3
            move_cursor_up()
        elseif pred == 4
            move_cursor_down()
        end
    end
end

# Continuous processing loop
while true
    chunk = await_next_eeg_chunk()  # Your data acquisition
    result = process_chunk(session, chunk)
    handle_cursor_control(result)
    
    # Check trial end condition
    if is_trial_complete()
        final = finalize_trial(session)
        println("Trial complete: $(final.prediction)")
        # Reset for next trial
    end
end
```

## Comparison: Batch vs Streaming

### Batch Inference

```julia
# Process all data at once (offline)
data = BCIData(features, metadata, labels)  # All trials at once
results = predict_batch(model, data)
```

**Best for:**
- Offline analysis
- Research datasets
- Multiple trials at once

### Streaming Inference

```julia
# Process chunk by chunk (online)
session = init_streaming(model, metadata)
for chunk in stream
    result = process_chunk(session, chunk)
end
final = finalize_trial(session)
```

**Best for:**
- Real-time control
- Live feedback
- Continuous monitoring

## Error Handling

```julia
try
    result = process_chunk(session, chunk)
catch e
    if isa(e, DataValidationError)
        @error "Invalid chunk data" size(chunk)
    elseif isa(e, ModelCompatibilityError)
        @error "Model incompatible with streaming config"
        # Try batch mode instead
    else
        rethrow(e)
    end
end
```

## Best Practices

### 1. Optimal Chunk Size

Choose chunk size based on your paradigm:

```julia
# Motor Imagery: 1-2 seconds
chunk_size = 250  # 1 second at 250 Hz

# P300: 1 second
chunk_size = 250

# SSVEP: 2-4 seconds
chunk_size = 500  # 2 seconds at 250 Hz
```

### 2. Confidence Thresholds

Set appropriate thresholds for your application:

```julia
# For safety-critical applications
threshold = 0.8

# For research/exploration
threshold = 0.6

if result.confidence < threshold
    # Reject or ask for user confirmation
end
```

### 3. Quality Monitoring

Monitor signal quality:

```julia
# Use preprocessing diagnostics
report = diagnose_preprocessing(data)

if report.quality_score < 0.7
    @warn "Signal quality low: $(report.recommendations)"
end
```

## Next Steps

<CardGroup cols={2}>
  <Card title="Batch Processing" icon="list" href="/inference-configuration/batch-processing">
    Offline batch inference for research data
  </Card>
  <Card title="Julia SDK Reference" icon="code" href="/api-reference/julia-sdk">
    Complete SDK API documentation
  </Card>
  <Card title="Real-time Setup" icon="rocket" href="/inference-configuration/real-time-setup">
    Configure real-time processing
  </Card>
  <Card title="Code Examples" icon="file-code" href="/examples/code-samples">
    Complete working examples
  </Card>
</CardGroup>

## Support

Need help with streaming inference?

- **Email**: hello@nimbusbci.com
- **Documentation**: https://docs.nimbusbci.com
- **SDK Repository**: https://github.com/nimbusbci/NimbusSDK.jl

Streaming inference is designed for production use with enterprise-grade performance and reliability.

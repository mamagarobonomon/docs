---
title: "Real-time Processing"
description: "How Nimbus achieves sub-25ms latency for responsive brain-computer interfaces"
icon: "clock"
---

# Real-time Processing for BCI

Real-time performance is critical for brain-computer interfaces. Users expect immediate responses to their neural commands - any delay breaks the sense of direct neural control and degrades the user experience. Both Python and Julia SDKs achieve 10-25ms inference latency, enabling truly responsive BCI applications.

<Note>
**Implementation differences:**
- **Python SDK**: Uses JAX/NumPyro for fast Bayesian inference
- **Julia SDK**: Uses RxInfer.jl with reactive message passing

Both achieve similar latencies (~10-25ms) with different technical approaches.
</Note>

## Why Latency Matters in BCI

### The 20ms Threshold

Research shows that for natural interaction, BCI systems need to respond within **20 milliseconds** of neural signal acquisition:

- **Motor BCIs**: Cursor control feels natural only with sub-20ms latency
- **Communication BCIs**: Real-time typing requires immediate character selection
- **Gaming BCIs**: Competitive gaming demands instant response to mental commands
- **Assistive BCIs**: Wheelchair control and robotic arms need immediate execution

<Warning>
Standard BCI processing pipelines take 200ms or more, creating a noticeable delay that breaks the illusion of direct neural control.
</Warning>

### Current BCI Latency Bottlenecks

Traditional BCI systems have multiple latency sources:

```
Signal Acquisition → Feature Extraction → Classification → Post-processing → Output
     ~10ms              ~50ms            ~100ms         ~40ms        ~10ms
                                    
Total: ~210ms (10x slower than needed)
```

**Major bottlenecks:**
- **Batch processing**: Waiting for signal windows (50-100ms)
- **Complex feature extraction**: FFTs, spatial filters, etc.
- **Inefficient classifiers**: SVMs, neural networks with high overhead
- **Post-processing**: Smoothing, voting, calibration steps

## Nimbus Real-time Architecture

### Streaming Inference Pipeline

NimbusSDK processes neural signals as continuous streams, not batches:

```
Neural Stream → Reactive Processing → Probabilistic Inference → Real-time Output
    ~1ms            ~5ms                    ~10-15ms            ~2ms

Total: 18-23ms (sub-25ms target achieved)
```

### Key Performance Features

<Columns cols={2}>
  <Card title="Reactive Message Passing" icon="refresh-cw">
    Updates only when new data arrives using RxInfer.jl
  </Card>
  <Card title="Incremental Inference" icon="arrow-right">
    Builds on previous computations instead of starting fresh
  </Card>
  <Card title="Optimized Factor Graphs" icon="network">
    Efficient graph structures minimize computation
  </Card>
  <Card title="Julia Performance" icon="settings">
    Native Julia compilation for maximum speed
  </Card>
</Columns>

## Technical Implementation

### Streaming Data Processing

NimbusSDK handles continuous data streams efficiently:

<Tabs>
  <Tab title="Python">
```python
from nimbus_pysdk import NimbusLDA, StreamingSession
from nimbus_pysdk.data import BCIMetadata
import numpy as np
import time

# Train a model
clf = NimbusLDA()
X_train = np.random.randn(100, 16)
y_train = np.random.randint(0, 4, 100)
clf.fit(X_train, y_train)

# Initialize streaming session
metadata = BCIMetadata(
    sampling_rate=250.0,
    paradigm="motor_imagery",
    feature_type="csp",
    n_features=16,
    n_classes=4,
    chunk_size=250  # 1 second chunks
)

session = StreamingSession(clf.model_, metadata)
print("Python streaming session initialized.")

# Simulate real-time data stream
num_chunks = 5
for i in range(num_chunks):
    start_time = time.perf_counter()
    chunk = np.random.randn(16, 250) # (n_features, chunk_size)
    chunk_result = session.process_chunk(chunk)
    end_time = time.perf_counter()
    latency_ms = (end_time - start_time) * 1000
    
    print(f"Python Chunk {i+1}: Pred={chunk_result.prediction}, Conf={chunk_result.confidence:.2f}, Latency={latency_ms:.2f}ms")
    
    # Immediate feedback to user
    if chunk_result.confidence > 0.7:
        # provide_immediate_feedback(chunk_result.prediction)
        pass

# Finalize with weighted voting across chunks
final_result = session.finalize_trial(method="weighted_vote")
print(f"\nPython Final prediction: {final_result.prediction}, Confidence={final_result.confidence:.2f}")
session.reset()
```
  </Tab>
  <Tab title="Julia">
```julia
using NimbusSDK

# Initialize streaming session
metadata = BCIMetadata(
    sampling_rate = 250.0,
    paradigm = :motor_imagery,
    feature_type = :csp,
    n_features = 16,
    n_classes = 4,
    chunk_size = 250  # 1 second chunks
)

session = init_streaming(model, metadata)

# Process chunks as they arrive - no waiting for full trials
for chunk in eeg_stream
    # Incremental processing - updates beliefs in ~15ms
    chunk_result = process_chunk(session, chunk)
    
    # Immediate feedback to user
    if chunk_result.confidence > 0.7
        provide_immediate_feedback(chunk_result.prediction)
    end
end

# Finalize with weighted voting across chunks
final_result = finalize_trial(session; method=:weighted_vote)
```
  </Tab>
</Tabs>

### Batch Processing for Offline Analysis

For offline analysis where latency isn't critical:

<Tabs>
  <Tab title="Python">
```python
from nimbus_pysdk import NimbusLDA, predict_batch
from nimbus_pysdk.data import BCIData, BCIMetadata
import numpy as np

# Train a model
clf = NimbusLDA()
X_train = np.random.randn(100, 16)
y_train = np.random.randint(0, 4, 100)
clf.fit(X_train, y_train)

# Prepare batch test data
X_test = np.random.randn(200, 16)
y_test = np.random.randint(0, 4, 200)

# Sklearn-compatible batch inference
predictions = clf.predict(X_test)
probabilities = clf.predict_proba(X_test)

# Typical performance: 200 trials in ~5 seconds
# Average latency per trial: ~25ms
accuracy = np.mean(predictions == y_test)
print(f"Batch accuracy: {accuracy:.1%}")
```
  </Tab>
  <Tab title="Julia">
```julia
using NimbusSDK

# Process multiple trials efficiently
results = predict_batch(model, bci_data; iterations=10)

# Typical performance: 200 trials in ~5 seconds
# Average latency per trial: ~25ms
```
  </Tab>
</Tabs>

### Reactive Message Passing

**RxInfer.jl** reactive programming principles:

- **Event-driven**: Computation triggered by new data arrival
- **Incremental**: Only update affected parts of the factor graph
- **Non-blocking**: Asynchronous message updates
- **Memory efficient**: Bounded memory usage for streaming

### Memory Efficiency

Real-time systems must manage memory carefully:

- **Bounded memory**: Fixed memory usage regardless of runtime
- **Efficient data structures**: Optimized for sequential access
- **Minimal allocation**: Reuse memory buffers when possible
- **Low GC pressure**: Julia's memory management optimized for real-time

## Performance Optimizations

### Algorithmic Optimizations

<AccordionGroup>
  <Accordion title="Sparse Factor Graphs">
    Most BCI models have sparse connectivity. NimbusSDK exploits this:
    
    - **Local updates**: Only affected nodes recompute
    - **Message caching**: Reuse previous computations
    - **Efficient scheduling**: Optimal update order
  </Accordion>
  
  <Accordion title="Variational Inference">
    RxInfer uses variational message passing for fast approximate inference:
    
    - **Closed-form updates**: No sampling required
    - **Parallel computation**: Independent message updates
    - **Convergence guarantees**: Provably correct inference
  </Accordion>
  
  <Accordion title="Adaptive Precision">
    Adjust computational precision based on requirements:
    
<Tabs>
  <Tab title="Python">
```python
from nimbus_pysdk import NimbusLDA, NimbusGMM

# NimbusLDA: Fast with shared covariance
# NimbusGMM: More flexible with class-specific covariances

# Choose based on accuracy/speed tradeoff
if accuracy_priority:
    clf = NimbusGMM(n_components=2)  # More flexible
else:
    clf = NimbusLDA()  # Faster
```
  </Tab>
  <Tab title="Julia">
```julia
# RxLDA: Fast with shared covariance
# RxGMM: More flexible with class-specific covariances

# Choose based on accuracy/speed tradeoff
model = accuracy_priority ? RxGMMModel : RxLDAModel
```
  </Tab>
</Tabs>
  </Accordion>
</AccordionGroup>

### Julia Performance

<Columns cols={2}>
  <Card title="JIT Compilation" icon="settings">
    First inference compiles, subsequent calls are fast
  </Card>
  <Card title="Type Specialization" icon="code">
    Compiler optimizes for actual data types
  </Card>
  <Card title="SIMD Operations" icon="cpu">
    Vectorized operations for array computations
  </Card>
  <Card title="Memory Efficiency" icon="database">
    Minimal allocation and efficient GC
  </Card>
</Columns>

## Real-World Performance

### Benchmark Results

NimbusSDK consistently achieves sub-25ms latency for BCI applications:

|| Application Type | Signal Type | Features | NimbusSDK Latency | Typical Pipeline |
||-----------------|-------------|----------|-------------------|------------------|
|| Motor Imagery   | CSP         | 16       | 15-20ms          | 180ms           |
|| P300 Detection  | ERP         | 12       | 12-18ms          | 150ms           |
|| Multi-class MI  | CSP         | 32       | 20-25ms          | 200ms           |
|| Binary MI       | CSP         | 8        | 10-15ms          | 120ms           |

### Scaling Characteristics

- **Feature count**: Linear scaling with number of features
- **Classes**: Minimal impact from 2-4 classes
- **Streaming**: Constant per-chunk latency
- **Batch**: Efficient parallel processing

<Note>
Performance measured on modern CPU (Intel i7 or equivalent). RxLDA is typically faster than RxGMM due to shared covariance structure.
</Note>

## Deployment Considerations

### Local Processing

For optimal latency, deploy NimbusSDK locally:

- **No network latency**: All processing on device
- **Predictable performance**: No cloud variability
- **Privacy**: Neural data stays local
- **Offline capability**: Works without internet

### System Requirements

<Tabs>
  <Tab title="Python SDK">

**Minimum:**
- Python 3.9+
- 4GB RAM
- Modern CPU (Intel i5 or equivalent)

**Recommended:**
- Python 3.10+
- 8GB RAM
- Fast CPU (Intel i7 or equivalent)
- SSD for data loading
</Tab>
  <Tab title="Julia SDK">

**Minimum:**
- Julia 1.9+
- 4GB RAM
- Modern CPU (Intel i5 or equivalent)

**Recommended:**
- Julia 1.10+
- 8GB RAM
- Fast CPU (Intel i7 or equivalent)
- SSD for model loading
</Tab>
</Tabs>

## Monitoring and Debugging

### Performance Metrics

Monitor system performance during inference:

<Tabs>
  <Tab title="Python">
```python
from nimbus_pysdk import OnlinePerformanceTracker
import numpy as np

# Track performance with built-in metrics
tracker = OnlinePerformanceTracker(window_size=50)

# Simulate predictions
predictions = np.random.randint(0, 4, 50)
true_labels = np.random.randint(0, 4, 50)
confidences = np.random.uniform(0.5, 1.0, 50)

for pred, label, conf in zip(predictions, true_labels, confidences):
    metrics = tracker.update_and_report(pred, label, conf)

# Get comprehensive metrics
full_metrics = tracker.get_metrics(n_classes=4, trial_duration=4.0)

print(f"Accuracy: {full_metrics['accuracy'] * 100:.1f}%")
print(f"ITR: {full_metrics['information_transfer_rate']:.1f} bits/min")
print(f"Mean confidence: {full_metrics['mean_confidence']:.3f}")
```
  </Tab>
  <Tab title="Julia">
```julia
using NimbusSDK

# Track performance with built-in metrics
tracker = OnlinePerformanceTracker(window_size=50)

for (pred, label, conf) in zip(predictions, true_labels, confidences)
    metrics = update_and_report!(tracker, pred, label, conf)
end

# Get comprehensive metrics
full_metrics = get_metrics(tracker, n_classes=4, trial_duration=4.0)

println("Accuracy: $(round(full_metrics.accuracy * 100, digits=1))%")
println("ITR: $(round(full_metrics.information_transfer_rate, digits=1)) bits/min")
println("Mean confidence: $(round(full_metrics.mean_confidence, digits=3))")
```
  </Tab>
</Tabs>

### Quality Assessment

Identify trials with poor signal quality:

<Tabs>
  <Tab title="Python">
```python
from nimbus_pysdk import assess_trial_quality, should_reject_trial

# Assess each trial's quality
# Assume 'result' is from predict_proba or StreamingSession
confidence = 0.65

quality = assess_trial_quality(confidence)

if not quality["confidence_acceptable"]:
    print(f"⚠️ Trial quality below threshold: {quality['overall_score']:.2f}")
    print(f"Recommendation: {quality['recommendation']}")

# Check if rejection is warranted
if should_reject_trial(confidence, threshold=0.7):
    print("⚠️  Low confidence trial - consider rejecting")
else:
    print("✓ Trial quality acceptable")
```
  </Tab>
  <Tab title="Julia">
```julia
using NimbusSDK

# Assess each trial's quality
quality = assess_trial_quality(results)

if !quality.confidence_acceptable
    @warn "Trial quality below threshold" quality.overall_score
    println("Recommendation: $(quality.recommendation)")
end

# Check if rejection is warranted
if should_reject_trial(confidence, 0.7)
    println("⚠️  Low confidence trial - consider rejecting")
end
```
  </Tab>
</Tabs>

<Tip>
Use quality assessment to identify when recalibration is needed or when environmental conditions are affecting signal quality.
</Tip>

## Getting Started with Real-time BCI

Ready to build ultra-fast BCI applications?

<CardGroup cols={2}>
  <Card title="Python SDK" icon="python" href="/python-sdk/quickstart">
    Get started with Python
  </Card>
  <Card title="Julia SDK" icon="code" href="/quickstart">
    Get started with Julia
  </Card>
  <Card title="Streaming Setup" icon="signal" href="/inference-configuration/real-time-setup">
    Configure streaming inference
  </Card>
  <Card title="Examples" icon="code" href="/examples/basic-examples">
    See real-time BCI in action
  </Card>
</CardGroup>

---

<Note>
**Next**: Learn how Nimbus handles [uncertainty in neural signals](/core-concepts/uncertainty-handling) to build robust BCI applications.
</Note>


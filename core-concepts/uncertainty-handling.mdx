---
title: "Uncertainty Handling"
description: "How Nimbus manages uncertainty in neural signals for robust BCI applications"
icon: "question"
---

# Uncertainty Handling in BCI

Neural signals are inherently uncertain. Signal quality varies, brain states change, and individual differences create significant variability. Traditional BCI systems ignore this uncertainty, leading to brittle performance and user frustration. Nimbus explicitly models and manages uncertainty to create robust, trustworthy BCI applications.

## Sources of Uncertainty in BCI

### Signal-Level Uncertainty

Neural recordings contain multiple sources of noise and variability:

<Columns cols={2}>
  <Card title="Measurement Noise" icon="signal">
    Electrical interference, amplifier noise, quantization errors
  </Card>
  <Card title="Biological Artifacts" icon="heartbeat">
    Eye blinks, muscle activity, cardiac signals, breathing
  </Card>
  <Card title="Environmental Factors" icon="globe">
    Temperature, humidity, electromagnetic interference
  </Card>
  <Card title="Electrode Issues" icon="link">
    Poor contact, impedance changes, electrode drift
  </Card>
</Columns>

### Cognitive Uncertainty

The brain itself introduces uncertainty:

- **State variability**: Attention, fatigue, mood affect neural patterns
- **Learning effects**: Brain patterns change as users adapt to the BCI
- **Individual differences**: Each person's brain is unique
- **Task complexity**: Difficult tasks produce more variable neural responses

### Model Uncertainty

BCI models have inherent limitations:

- **Training data**: Limited samples may not capture full variability
- **Model assumptions**: Simplified models miss complex neural dynamics
- **Generalization**: Performance on new users or conditions is uncertain
- **Temporal drift**: Models become outdated as brain patterns evolve

## Traditional Approaches vs. Nimbus

### Deterministic BCI Systems

Most current BCI systems ignore uncertainty:

```python
# Traditional approach - no uncertainty handling
def classify_intent(eeg_signal):
    features = extract_features(eeg_signal)
    prediction = classifier.predict(features)
    return prediction  # Always returns an answer, no confidence measure
```

**Problems:**
- **Overconfident**: Always provides an answer, even with poor signal quality
- **No adaptation**: Cannot adjust behavior based on uncertainty
- **Poor user experience**: Users can't tell when the system is struggling
- **Safety issues**: Critical applications need to know when predictions are unreliable

### Nimbus Probabilistic Approach

Nimbus models uncertainty explicitly:

```python
# Nimbus approach - uncertainty-aware
def infer_intent(eeg_signal):
    # Returns full probability distribution
    intent_posterior = nimbus.infer(
        signal=eeg_signal,
        model=bci_model,
        return_uncertainty=True
    )
    
    return {
        'prediction': intent_posterior.mode(),
        'confidence': intent_posterior.confidence(),
        'entropy': intent_posterior.entropy(),
        'alternatives': intent_posterior.top_k(3)
    }
```

## Types of Uncertainty

### Aleatoric Uncertainty (Data Uncertainty)

Irreducible uncertainty inherent in the data:

```python
# Example: Noisy EEG signal
signal_quality = nimbus.assess_signal_quality(eeg_data)

if signal_quality.snr < threshold:
    # High aleatoric uncertainty due to noise
    response = {
        'prediction': result.mode(),
        'confidence': 'low',
        'reason': 'Poor signal quality detected'
    }
```

**Characteristics:**
- Cannot be reduced by more data or better models
- Varies across time and conditions
- Requires adaptive responses

### Epistemic Uncertainty (Model Uncertainty)

Reducible uncertainty due to limited knowledge:

```python
# Example: New user with limited training data
training_samples = get_user_training_data(user_id)

if len(training_samples) < minimum_samples:
    # High epistemic uncertainty - need more data
    response = {
        'prediction': result.mode(),
        'confidence': 'low',
        'reason': 'Insufficient training data for this user'
    }
```

**Characteristics:**
- Can be reduced with more training data
- Higher for new users or novel conditions
- Decreases as the model learns

## Uncertainty Quantification Methods

### Confidence Measures

Nimbus provides multiple ways to quantify confidence:

<AccordionGroup>
  <Accordion title="Posterior Entropy">
    Measures how "spread out" the probability distribution is:
    
    ```python
    entropy = intent_posterior.entropy()
    if entropy > high_uncertainty_threshold:
        # Very uncertain - multiple possibilities
        request_clarification()
    ```
  </Accordion>
  
  <Accordion title="Maximum Probability">
    The probability of the most likely prediction:
    
    ```python
    max_prob = intent_posterior.max()
    if max_prob < confidence_threshold:
        # Low confidence in top prediction
        show_alternatives()
    ```
  </Accordion>
  
  <Accordion title="Prediction Intervals">
    Range of plausible values for continuous outputs:
    
    ```python
    cursor_position = nimbus.predict_cursor_position(eeg_signal)
    confidence_interval = cursor_position.confidence_interval(0.95)
    
    # Show uncertainty visualization to user
    display_cursor_with_uncertainty(cursor_position, confidence_interval)
    ```
  </Accordion>
</AccordionGroup>

### Bayesian Model Averaging

Combine multiple models to capture model uncertainty:

```python
# Ensemble of BCI models
models = [motor_imagery_model, p300_model, ssvep_model]

# Weighted combination based on signal characteristics
ensemble_prediction = nimbus.model_average(
    models=models,
    signal=eeg_data,
    weights='adaptive'  # Automatically weight based on signal type
)
```

## Adaptive Responses to Uncertainty

### Confidence-Based Actions

Adjust system behavior based on uncertainty levels:

```python
def adaptive_bci_response(prediction_result):
    confidence = prediction_result.confidence()
    
    if confidence > 0.9:
        # High confidence - execute immediately
        execute_command(prediction_result.prediction)
        
    elif confidence > 0.7:
        # Medium confidence - show options
        show_confirmation_dialog(
            primary=prediction_result.prediction,
            alternatives=prediction_result.alternatives
        )
        
    elif confidence > 0.5:
        # Low confidence - request clearer signal
        display_message("Please focus and try again")
        
    else:
        # Very low confidence - pause and recalibrate
        suggest_recalibration()
```

### Dynamic Thresholds

Adjust decision thresholds based on context:

```python
# Safety-critical application (wheelchair control)
if application_context == 'wheelchair_navigation':
    confidence_threshold = 0.95  # Very high threshold for safety
    
# Gaming application
elif application_context == 'gaming':
    confidence_threshold = 0.7   # Lower threshold for responsiveness
    
# Communication aid
elif application_context == 'communication':
    confidence_threshold = 0.8   # Balanced threshold
```

## User Interface for Uncertainty

### Visualizing Confidence

Help users understand system confidence:

```python
# Visual confidence indicators
def display_prediction_with_confidence(prediction, confidence):
    if confidence > 0.8:
        color = 'green'
        message = 'High confidence'
    elif confidence > 0.6:
        color = 'yellow'  
        message = 'Medium confidence'
    else:
        color = 'red'
        message = 'Low confidence - please focus'
    
    display_prediction(prediction, color=color, message=message)
```

### Progressive Disclosure

Show more information when uncertainty is high:

```python
if prediction_result.entropy() > uncertainty_threshold:
    # Show alternative predictions
    display_alternatives(prediction_result.top_k(3))
    
    # Show confidence scores
    display_confidence_bars(prediction_result.probabilities)
    
    # Suggest actions to improve signal
    suggest_improvements(signal_quality_assessment)
```

## Clinical and Safety Applications

### FDA Compliance

For medical BCI devices, uncertainty quantification is crucial:

<Note>
FDA guidance emphasizes the importance of understanding when AI systems are uncertain. Probabilistic BCI systems provide the transparency needed for regulatory approval.
</Note>

```python
# Medical-grade uncertainty reporting
def generate_clinical_report(bci_session):
    report = {
        'session_id': bci_session.id,
        'predictions': bci_session.predictions,
        'confidence_scores': bci_session.confidence_scores,
        'uncertainty_flags': bci_session.uncertainty_events,
        'signal_quality_metrics': bci_session.signal_quality,
        'model_version': bci_session.model_version
    }
    
    # Include uncertainty analysis
    report['uncertainty_analysis'] = analyze_uncertainty_patterns(report)
    
    return report
```

### Risk Management

Use uncertainty for risk assessment:

```python
def assess_risk_level(prediction_result, application_context):
    confidence = prediction_result.confidence()
    
    # High-risk actions require high confidence
    if application_context.is_safety_critical():
        if confidence < 0.95:
            return RiskLevel.HIGH
    
    # Standard risk assessment
    if confidence < 0.5:
        return RiskLevel.HIGH
    elif confidence < 0.7:
        return RiskLevel.MEDIUM
    else:
        return RiskLevel.LOW
```

## Advanced Uncertainty Techniques

### Active Learning

Use uncertainty to guide data collection:

```python
# Identify uncertain regions for additional training
uncertain_samples = nimbus.find_uncertain_regions(
    user_data=training_data,
    uncertainty_threshold=0.6
)

# Request additional training on uncertain cases
for sample in uncertain_samples:
    request_user_feedback(sample)
```

### Uncertainty Calibration

Ensure confidence scores are well-calibrated:

```python
# Calibrate confidence scores on validation data
calibrator = nimbus.UncertaintyCalibrator()
calibrator.fit(validation_predictions, validation_labels)

# Apply calibration to new predictions
calibrated_confidence = calibrator.transform(raw_confidence)
```

## Getting Started with Uncertainty

Ready to build uncertainty-aware BCI applications?

<CardGroup cols={2}>
  <Card title="Probabilistic Models" icon="lightbulb" href="/core-concepts/probabilistic-ai">
    Learn about probabilistic AI for BCI
  </Card>
  <Card title="API Reference" icon="code" href="/api-reference/inference-endpoints">
    Explore uncertainty-aware APIs
  </Card>
  <Card title="Examples" icon="list" href="/examples/basic-examples">
    See uncertainty handling in practice
  </Card>
  <Card title="Best Practices" icon="check" href="/development/testing-validation">
    Guidelines for uncertainty-aware BCI
  </Card>
</CardGroup>

---

<Note>
**Next**: Learn about [message passing architecture](/core-concepts/message-passing) that enables efficient probabilistic inference in Nimbus.
</Note>
